# 使用 pyquery（一个 Python 库）进行抓取

从本章开始，我们将探索与刮片相关的工具和技术，同时我们还将部署一些刮片代码。与 web 探索、Python 库、元素识别和遍历相关的特性是我们迄今为止学习的主要概念。

网页抓取通常是一个具有挑战性的长期过程，需要了解网站的运行情况。了解和识别用于构建网站的后端或工具的基本能力将有助于完成任何刮削任务。这也与称为逆向工程的过程有关。有关此类工具的更多信息，请参考[第 3 章](03.html)、*使用 LXML、XPath 和 CSS 选择器*以及*使用 web 浏览器开发工具访问 web 内容*部分。除此之外，还需要识别用于遍历和操作 HTML 标记等元素的工具，`pyquery`就是其中之一。

在前面的章节中，我们探讨了 XPath、CSS 选择器和 LXML。在本章中，我们将探讨如何使用`pyquery`，它具有类似 jQuery 的功能，似乎更高效，因此在涉及到 web 抓取过程时更容易处理。

在本章中，您将了解以下主题：

*   `pyquery`简介
*   探索`pyquery`（主要方法和属性）
*   使用`pyquery`刮网

# 技术要求

本章要求使用网络浏览器（Google Chrome 或 Mozilla Firefox）。我们将使用以下 Python 库：

*   `pyquery`
*   `urllib`
*   `requests`

如果您当前的 Python 设置中不存在这些库，请参阅[第 2 章](02.html)、*Python 和 Web–使用 urllib 和请求*以及*设置内容*部分，以获取安装和设置帮助。

本章的代码文件可在本书的 GitHub 存储库中找到：[https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter04) 。

# pyquery 简介

`pyquery`是一个类似 jQuery 的 Python 库，它使用`lxml`库。这为处理标记元素提供了一个简单的交互式环境，用于操作和遍历目的。

`pyquery` expressions are also similar to `jquery`, and users with `jquery` knowledge will find it more convenient to use in Python.

顾名思义，`pyquery`Python 库增强了与 XML 和 HTML 中元素相关的`query`编写过程。`pyquery`缩短了元素处理，并提供了一种更具洞察力的脚本编写方法，适用于抓取和基于 DOM 的遍历和操作任务。

`pyquery`表达式使用 CSS 选择器执行查询，以及它实现的其他功能。例如，`pyquery`使用以下表达式：

```py
    page.find('a').attr('href')    -- (pyquery expression)      
```

`cssselect`使用以下表达式：

```py
    cssselect('a').get('href')      -- (cssselect expression)    
```

jQuery (write less, do more) is one of the most admired JavaScript libraries and is small, quick, and has lots of features that support DOM/HTML/CSS, and more. Web document-based traversing, manipulation, event handling, animation, AJAX, and more are some of its main features. Please visit [https://jquery.com/](https://jquery.com/) for more information. For more information on `pyquery` and its documentation, please visit [https://pythonhosted.org/pyquery/](https://pythonhosted.org/pyquery/) or [https://github.com/gawel/pyquery/](https://github.com/gawel/pyquery/).

# 探索 pyquery

在我们继续探索`pyquery`及其功能之前，让我们先使用`pip`安装它：

```py
C:\> pip install pyquery
```

For more information on using `pip` and library installation, please refer to the *Setting things up* section in [Chapter 2](02.html), *Python and the Web – Using urllib and Requests*.

使用`pip`成功安装`pyquery`时，将安装以下库：

*   `cssselect-1.0.3`
*   `lxml-4.3.1`
*   `pyquery-1.4.0`

`>>>` in the code represents the use of the Python IDE; it accepts the code or instructions and displays the output on the next line.

一旦安装完成并成功，我们可以使用`pyquery`，如下代码所示，来确认设置。我们可以使用`dir()`函数来探索它包含的属性：

```py
>>> from pyquery import PyQuery as pq

>>> print(dir(pq))
['Fn', '__add__', '__call__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__',  '_filter_only', '_get_root', '_next_all', '_prev_all', '_translator_class', '_traverse','addClass', 'add_class', 'after', 'append', 'appendTo', 'append_to','attr','base_url','before','children', 'clear', 'clone', 'closest', 'contents', 'copy', 'count', 'css','each','empty', 'encoding','end','eq', 'extend', 'filter', 'find','fn','hasClass','has_class','height','hide', 'html', 'index','insert','insertAfter', 'insertBefore', 'insert_after','insert_before', 'is_', 'items', 'length','make_links_absolute',
'map','next','nextAll','next_all','not_','outerHtml','outer_html','parent','parents', 'pop', 'prepend', 'prependTo', 'prepend_to','prev', 'prevAll', 'prev_all', 'remove', 'removeAttr', 'removeClass', 'remove_attr', 'remove_class','remove_namespaces', 'replaceAll', 'replaceWith', 'replace_all', 'replace_with', 'reverse', 'root','show', siblings','size','sort','text', 'toggleClass', 'toggle_class', 'val', 'width', 'wrap', 'wrapAll','wrap_all','xhtml_to_html']
```

现在，我们将探索`pyquery`中与刮削概念相关的某些特性。为此，我们将使用可从[获得的页面源 https://www.python.org](https://www.python.org) 已在本地保存为`test.html`以提供真实可用性：

![](assets/3ab8aac9-7322-44af-8a01-d3fd6723b621.png)

Page source obtained from https://www.python.org In Google Chrome, you can right-click on the web page and choose the View page source menu option or press *Ctrl* + *U* to obtain the page source.

但是，仅仅获取页面源代码或 HTML 代码是不够的，因为我们需要将这些内容加载到库中，以获得更多的工具来进行探索。我们将在下一节中进行介绍。

While testing or following the code, you might find or require changes to be done on the `pyquery` code expressions in order to obtain the real output. Page sources that are obtained now might be updated or changed. You are suggested to obtain the latest page source from the source URL ([https://www.python.org](https://www.python.org)).

# 装载文件

在大多数情况下，文档内容通过使用`requests`或`urllib`获得，并提供给`pyquery`如下：

```py
>>> from pyquery import PyQuery as pq
>>> import requests
>>> response = requests.get('http://www.example.com').text #content

>>> from urllib.request import urlopen
>>> response = urlopen('http://www.example.com').read()
>>> docTree = pq(response)
```

`pyquery`还可以使用 Python 库`urllib`（默认）或请求加载 URL。它还支持基于请求的参数：

```py
>>> pq("https://www.python.org")
[<html.no-js>] 

>>> site=pq("https://www.python.org")
>>> print(type(site))
<class 'pyquery.pyquery.PyQuery'> 

>>> pq("https://www.samsclub.com")
[<html>]
```

我们从前面代码中获得的`pq`对象正在使用`lxml`提供的 XML 解析器（默认）进行解析，该解析器也可以通过传递给它的额外`parser`参数进行更新：

```py
>>> doc = pq('http://www.exaple.com', parser = 'xml')  #using parser xml

>>> doc = pq('http://www.exaple.com', parser = 'html') #using parser html
```

通常，来自页面源或其他源（如文件）的 HTML 代码作为字符串提供给`pyquery`进行进一步处理，如下代码所示：

```py
>>> doc = pq('<div><p>Testing block</p><p>Second block</p></div>')
>>> print(type(doc))
<class 'pyquery.pyquery.PyQuery'>

>>> pagesource = open('test.html','r').read() #reading locally saved HTML
>>> print(type(pagesource))
<class 'str'>

>>> page = pq(pagesource)
>>> print(type(page))
<class 'pyquery.pyquery.PyQuery'>
```

通过从加载的文档或 URL 接收的`PyQuery`对象或`pq`，我们可以继续探索`pyquery`提供的功能。

# 元素遍历、属性和伪类

`pyquery`拥有大量的属性和方法，可以部署这些属性和方法来获取所需的内容。在以下示例中，我们将根据本节中的代码确定实现：

```py
>>> page('title') #find element <title>
[<title>]

>>> page.find('title').text() #find element <title> and return text content
'Welcome to Python.org'

>>> page.find('meta[name="description"]').attr('content')
'The official home of the Python Programming Language'

>>> page.find('meta[name="keywords"]').attr('content')
'Python programming language object oriented web free open source software license documentation download community'

>>> buttons = page('a.button').html() #return HTML content for element <a> with class='button'
>>> buttons
'>_\n <span class="message">Launch Interactive Shell</span>\n ' 
```

以下是它们的一些功能，以及可以在前面的代码中看到的描述：

*   `find()`：使用 CSS 选择器搜索提供的元素或计算查询表达式构建
*   `text()`：以字符串形式返回元素内容
*   `attr()`：标识属性并返回其内容
*   `html()`：返回计算表达式的 HTML 内容

The `class` and `id` CSS attributes are represented with `.` and `#`, respectively, and are prefixed to the attribute's value. For example, `<a class="main" id="mainLink">` will be identified as `a.main` and `a#mainLink`.

在下面的代码中，我们列出了所有已识别的具有`class`属性和`menu`值的`<ul>`元素：

```py
>>> page('ul.menu') #<ul> element with attribute class='menu'
[<ul.menu>, <ul.navigation.menu>, <ul.subnav.menu>, <ul.navigation.menu>, <ul.subnav.menu>, <ul.navigation.menu>,..............,    <ul.subnav.menu>, <ul.footer-        links.navigation.menu.do-not-print>] 
```

表达式被传递给 PyQuery 对象，该对象生成一个已计算元素的列表。对这些元素进行迭代以获取其精确值或内容。

PyQuery 还包含伪类或`:pseudo element`，用于索引和获取预定义的表达式结果。`:pseudo element`也可以附加到现有选择器查询。以下代码实现了遍历时常见的一些伪元素：

```py
>>> page('nav:first') #first <nav> element
[<nav.meta-navigation.container>]

>>> page('a:first') #first <a> element
[<a>]

>>> page('ul:first') #first <ul> element
[<ul.menu>]

>>> page('ul:last') #last <ul> element
[<ul.footer-links.navigation.menu.do-not-print>]
```

让我们看一下前面代码中使用的伪元素：

*   `:first`：返回所提供内容中第一次出现的元素
*   `:last`：返回所提供内容中最后出现的元素

让我们看看更多`:pseudo element`的一般实现，以列出 HTML 元素：

```py
>>> page(':header') #header elements found 
[<h1.site-headline>, <h1>, <h1>, <h1>, <h1>, <h1>, <h2.widget-title>, <h2.widget-title>..........,<h2.widget-title>, <h2.widget-title>, <h2.widget-title>]

>>> page(':input') #input elements found
[<input#id-search-field.search-field>, <button#submit.search-button>]

>>> page(':empty') #empty elements found
[<meta>, <meta>, <link>, <meta>, <meta>, <meta>, <meta>,<script>, <link>, <link>,........,<img.python-logo>, <span.icon-search>,<span.icon-facebook>, <span.icon-twitter>, <span.icon-freenode>, ...........,<span.icon-feed>, <div.python-logo>, <span#python-status-indicator.python
-status-indicator-default>, <script>, <script>, <script>]

>>> page(':empty:odd') #empty elements, only Odd ones are listed
[<meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <script>, <link>, <link>, <link>, <link>, <meta>, .......,<img.python-logo>, <span.icon-google-plus>, <span.icon-twitter>, <span.breaker>, <span.icon-download>, <span.icon-jobs>, <span.icon-calendar>, <span.icon-python>, <div.python-logo>, <script>,<script>]
```

下面是我们在前面代码中使用的`:pseudo element`：

*   `:header`：返回在页面中找到的标题元素（*h1、h2、…、h5、h6*）。

*   `:input`：返回所有输入元素。存在大量基于 HTML`<form>`的伪元素。请参考[https://pythonhosted.org/pyquery/](https://pythonhosted.org/pyquery/) 了解更多信息。

*   `:empty`：返回所有没有子元素的元素。

*   `:odd`：返回索引为奇数的元素。它们可以与其他`:pseudo element`一起使用，如`:empty:odd`。

*   `:even`：与`:odd`类似，但返回索引均匀的元素。

下面的代码演示了一起遍历`:pseudo element`和元素属性的表达式：

```py
>>> page.find('ul:first').attr('class') #class name of first <ul> element
'menu'

>>> page.find('a:first').attr('href') #href value of first <a> element
'#content'

>>> page.find('a:last').attr('href') #href value of last <a> element
'/psf/sponsorship/sponsors/'

>>> page.find('a:eq(0)').attr('href') #href value of first <a> element using Index!
'#content'

>>> page.find('a:eq(0)').text() #text from first <a> element
'Skip to content' 
```

以下是更多的`:pseudo element`。我们可以使用这些来解决元素的`index`：

*   `:eq`：选择特定的索引号；计算结果为`equals to`。
*   `:lt`：提供的索引号计算为`less than`。例如，`page('a:lt(2)')`。
*   `:gt`：提供的索引号评估为`greater than`。例如，`page('a:gt(0)')`。

除了用于识别索引和查找元素的一般功能外，`:pseudo element`还可以使用提供的文本搜索元素，如下代码所示：

```py
>>> page('p:contains("Python")') #return elements <p> with text 'Python"
[<p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>]

>>> page('p:contains("python.org")') #return elements <p> with text "python.org"
[<p>, <p>]

#return text from second <p> element containing text "python.org"
>>> page('p:contains("python.org")').eq(1).text() 
'jobs.python.org'
```

以下列表描述了前面代码中使用的`:contains`和`eq()`的简单定义：

*   `:contains`：匹配包含所提供文本的所有元素。
*   `eq()`：返回为特定索引编号找到的元素。评估为`equals to`，与`:eq`类似。

`pyquery`有几个返回布尔答案的函数，在需要搜索具有属性的元素并确认属性值的情况下非常有效：

```py
 #check if class is 'python-logo'        
>>> page('h1.site-headline:first a img').is_('.python-logo') 
 *True* 

 #check if <img> has class 'python-logo'
        >>> page('h1.site-headline:first a img').has_class('python-logo') 
 *True* 
```

以下是以前代码中使用的函数及其定义：

*   `is_()`：接受选择器作为参数，如果选择器匹配元素，则返回`True`，否则返回`False`。
*   `has_class()`：如果选择器与提供的类匹配，则返回`True`。它对于识别具有`class`属性的元素非常有用。

我们在`pyquery`中使用了一些重要的函数和工具，以增强元素标识和遍历相关属性。在下一节中，我们将学习并演示迭代。

# 迭代

在本节中，我们将演示`pyquery`提供的迭代（重复执行）功能。它在许多情况下都是有效且易于处理的。

在下面的代码中，我们正在搜索在包含单词`Python.org`的`<meta>`标记中找到的`name`和`property`属性。我们还使用 Python 的`List Comprehension`技术来演示单行编码功能：

```py
#Find <meta> with attribute 'content' containing '..Python.org..' 
#and list the attribute 'name' that satisfies the find()

>>> meta=page.find('meta[content*="Python.org"]')
>>> [item.attr('name') for item in meta.items() if item.attr('name') is not None]
['application-name', 'apple-mobile-web-app-title']

#Continuing from code above list value for attribute 'property'

>>> [item.attr('property') for item in meta.items() if item.attr('property') is not None]
['og:site_name', 'og:title']
```

正如我们在前面的代码中所看到的，我们在循环中使用`items()`函数和元素 meta 来迭代所提供的选项。可使用`items()`探索产生可编辑对象的表达式。返回`None`的结果不在列表中：

```py
>>> social = page.find('a:contains("Socialize") + ul.subnav li a') 
>>> [item.text() for item in social.items() if item.text() is not None]
['Google+', 'Facebook', 'Twitter', 'Chat on IRC']

>>> [item.attr('href') for item in social.items() if item.attr('href') is not None]
['https://plus.google.com/+Python', 'https://www.facebook.com/pythonlang?fref=ts', 'https://twitter.com/ThePSF', '/community/irc/']

 >>> webdevs = page.find('div.applications-widget:first ul.menu li:contains("Web Development") a') 
 >>> [item.text() for item in webdevs.items() if item.text() is not None] 
 ['Django', 'Pyramid', 'Bottle', 'Tornado', 'Flask', 'web2py'] 
```

在前面的代码中，`pyquery`对象收集社交和 web 开发部分提供的名称和链接。这些可以在使用 Python for。。。在下面的屏幕截图中。使用 Python 列表理解技术迭代对象：

![](assets/2fa561d2-c2e5-446b-9e2a-0fbc729fdb29.png)

Upcoming events to be extracted using pyquery

在下面的代码中，我们将探索从`upcomingevents`迭代中检索到的更多细节：

```py
>>> eventsList = []
>>> upcomingevents = page.find('div.event-widget ul.menu li')
>>> for event in upcomingevents.items():
 ...     time = event.find('time').text()
 ...     url = event.find('a[href*="events/python"]').attr('href')
 ...     title = event.find('a[href*="events/python"]').text()
 ...     eventsList.append([time,title,url])
 ...
>>> eventsList
```

`eventsList`包含从即将到来的事件中提取的细节，如前面的屏幕截图所示。此处提供`eventsList`的输出：

```py
[['2019-02-19', 'PyCon Namibia 2019', '/events/python-events/790/'], ['2019-02-23', 'PyCascades 2019', '/events/python-events/757/'],
['2019-02-23', 'PyCon APAC 2019', '/events/python-events/807/'], ['2019-02-23', 'Berlin Python Pizza', '/events/python-events/798/'],
['2019-03-15', 'Django Girls Rivers 2019 Workshop', '/events/python-user-group/816/']]
```

DevTools can be used to identify a CSS selector for the particular section and can be further processed with the looping facility. For more information regarding the CSS Selector, please refer to [Chapter 3](03.html), *Using LXML, XPath, and CSS Selectors*, *and the* *XPath and CSS selectors using DevTools* section.

下面的代码举例说明了通过使用`find()`和`items()`的`pyquery`迭代过程：

```py
>>> buttons = page.find('a.button')
>>> for item in buttons.items():
...     print(item.text(),' :: ',item.attr('href'))
...

>_ Launch Interactive Shell  ::  /shell/
Become a Member  ::  /users/membership/
Donate to the PSF  ::  /psf/donations/

>>> buttons = page.find('a.button:odd')
>>> for item in buttons.items():
...     print(item.text(),' :: ',item.attr('href'))
...

Become a Member  ::  /users/membership/

>>> buttons = page.find('a.button:even')
>>> for item in buttons.items():
...     print(item.text(),' :: ',item.attr('href'))
...

>_ Launch Interactive Shell  ::  /shell/
Donate to the PSF  ::  /psf/donations/
```

有关`pyquery`中的特性、属性和方法的更多信息，请参考[https://pythonhosted.org/pyquery/index.html](https://pythonhosted.org/pyquery/index.html) 。

# 使用 pyquery 进行 Web 抓取

在上一节中，我们学习了如何使用`pyquery`中提供的一些重要特性，并使用这些特性遍历或识别元素。在本节中，我们将使用`pyquery`中的大部分功能，并通过提供各种用例的示例，使用它们从 web 上获取数据。

# 示例 1–抓取数据科学公告

在本例中，我们将从[中删除数据科学类别中的公告相关详细信息 https://developer.ibm.com/announcements/ 类别/数据科学/](https://developer.ibm.com/announcements/category/data-science/)。

The same URL from [https://developer.ibm.com/](https://developer.ibm.com/) has also been used to collect data using `lxml.cssselect` under *Example 3*, in the *Web scraping using LXML* section from [Chapter 3](03.html), *Using LXML, XPath, and CSS Selectors*. It is suggested that you explore both examples and compare the features that were used.

首先，我们导入`pyquery`和`requests`：

```py
    from     pyquery     import     PyQuery     as     pq
    import     requests
dataSet =     list    ()
```

创建`dataSet`以便您有一个空列表来收集我们将从各个页面中找到的数据，以及要使用的库。我们已经声明了`read_url()`，它将用于读取提供的 URL 并返回`PyQuery`对象。在本例中，我们将使用`sourceUrl`，即[https://developer.ibm.com/announcements/](https://developer.ibm.com/announcements/) ：

```py
sourceUrl=    'https://developer.ibm.com/announcements/'

    def     read_url(url):
     """Read given Url , Returns pyquery object for page content"""
             pageSource = requests.get(url).content
     return     pq(pageSource)
```

要收集的信息可从[中检索 https://developer.ibm.com/announcements/category/data-science/?fa=date:DESC &fb=](https://developer.ibm.com/announcements/category/data-science/?fa=date:DESC&fb=)或使用`sourceUrl+"category/data-science/?fa=date:DESC&fb="`获取。在这里，我们将通过`pageUrls`进行循环。

`pageUrls`产生以下页面 URL。这些是通过列表理解和`range()`获得的：

*   [https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC &fb=](https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC&fb=)
*   [https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC &fb=](https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC&fb=)

如下面的代码所示，`pageUrls`生成一个基于页面的 URL 列表，可以通过使用`get_details()`函数进一步处理这些 URL。这用于检索文章：

```py
    if     __name__ ==     '__main__'    :
    mainUrl = sourceUrl+    "category/data-science/?fa=date:DESC&fb="
                pageUrls = [sourceUrl+    "category/data-science/page/%(page)s?fa=date:DESC&fb="     % {    'page'    : page}     for     page     in         range    (    1    ,     3    )]

        for     pages     in     pageUrls:
        get_details(pages)

        print    (    "\nTotal articles collected: "    ,     len    (dataSet))
        print    (dataSet)
```

从前面的代码中可以看出，列出了以下 URL：

*   [https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC &fb=](https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC&fb=)
*   [https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC &fb=](https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC&fb=)

`pageUrls`中的 URL 被迭代并传递给`get_details()`进行进一步处理，如下代码所示：

```py
    def     get_details(page):
        """read 'page' url and append list of queried items to dataSet"""
                response = read_url(page)

    articles = response.find(    '.ibm--card > a.ibm--card__block_link'    )
        print    (    "\nTotal articles found :"    , articles.    __len__    (),     ' in Page: '    , page)

        for     article     in     articles.items():
        link = article.attr(    'href'    )
        articlebody = article.find(    'div.ibm--card__body'    )

        adate = articlebody.find(    'h5 > .ibm--card__date'    ).text()
        articlebody.find(    'h5 > .ibm--card__date'    ).remove()
        atype = articlebody.find(    'h5'    ).text().strip()
        title = articlebody.find(    'h3.ibm--card__title'    ).text().encode(    'utf-8'    )
        excerpt = articlebody.find(    'p.ibm--card__excerpt'    ).text().encode(    'utf-8'    )
        category = article.find(    'div.ibm--card__bottom > p.cpt-byline__categories span'    )

            if     link:
            link =     str    (link).replace(    '/announcements/'    , mainUrl)
            categories = [span.text     for     span     in     category     if     span.text !=     '+'    ]
            dataSet.append([link, atype, adate, title, excerpt,    ","    .join(categories)])
```

`read_url()`读取传递给`get_details()`的页面 URL，并从`PyQuery`对象获取`response`。使用 CSS 选择器将包含块的信息标识为项目。因为有不止一个`articles`迭代可用，所以我们使用`items()`。然后在清理、替换和合并活动的帮助下对单个数据元素进行处理，然后再将其附加到主数据集，在本例中为`dataSet`。PyQuery 表达式也可以通过使用`articlebody`来缩短。

另外，使用`remove()``PyQuery`（操作）方法移除`<h5>`中的`.ibm--card__date`，以获得`atype`。`atype`内容还将包含额外的`.ibm--card__date`详细信息，如果使用时未删除以下代码：

```py
articlebody.find(    'h5 > .ibm--card__date'    ).remove())
```

从前面的代码中获得的最终输出如下：

```py
Total articles found : 8 in Page: https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC&fb=

Total articles found : 2 in Page: https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC&fb=

Total articles collected: 10

[['https://developer.ibm.com/announcements/model-mgmt-on-watson-studio-local/', 'Announcement', 'Nov 05, 2018', b'Perform feature engineering and model scoring', b'This code pattern demonstrates how data scientists can leverage IBM Watson Studio Local to automate the building and training of\xe2\x80\xa6', 'Analytics,Apache Spark'], ..........................., ['https://developer.ibm.com/announcements/algorithm-that-gives-you-answer-to-any-particular-question-based-on-mining-documents/', 'Announcement', 'Sep 17, 2018', b'Query a knowledge base to get insights about data', b'Learn a strategy to query a knowledge graph with a question and find the right answer.', 'Artificial Intelligence,Data Science'], ['https://developer.ibm.com/announcements/build-a-domain-specific-knowledge-graph-from-given-set-of-documents/', 'Announcement', 'Sep 14, 2018', b'Walk through the process of building a knowledge base by mining information stored in the documents', b'Take a look at all of the aspects of building a domain-specific knowledge graph.', 'Artificial Intelligence,Data Science']]
```

# 示例 2–从嵌套链接中抓取信息

在本例中，我们将从[的书籍中获取引用的详细信息 http://quotes.toscrape.com/tag/books/](http://quotes.toscrape.com/tag/books/) 。每个单独的报价都包含某些信息，以及指向作者详细信息页面的链接，我们还将对其进行处理，以便获得有关作者的信息：

![](assets/baf63b98-caa9-42bd-9bb1-2a98431ed559.png)

Main page from http://quotes.toscrape.com/tag/books/

在下面的代码中，`keys`中的元素将用作输出键，并包含 Python 字典。基本上，我们将收集键中元素的数据：

```py
    from     pyquery     import     PyQuery     as     pq
sourceUrl =     'http://quotes.toscrape.com/tag/books/'
    dataSet =     list    ()
keys = [    'quote_tags'    ,    'author_url'    ,    'author_name'    ,    'born_date'    ,    'born_location'    ,    'quote_title'    ]

    def     read_url(url):
        """Read given Url , Returns pyquery object for page content"""
                pageSource = pq(url)
        return     pq(pageSource)
```

前面代码中的`read_url()`也进行了更新，与*示例 1–抓取数据科学公告*部分中使用的库不同。在本例中，它返回所提供 URL 的 PyQuery 对象：

```py
    if     __name__ ==     '__main__'    :
    get_details(sourceUrl)

        print    (    "        \n        Total Quotes collected: "    ,     len    (dataSet))
        print    (dataSet)

        for     info     in     dataSet:
            print    (info[    'author_name'    ],    ' born on '    ,info[    'born_date'    ],     ' in '    ,info[    'born_location'    ])
```

对于`info`字典中的某些值，`dataSet`正在进行额外的迭代，该字典位于`dataSet`中。

如下代码所示，`get_details()`使用`while`循环进行分页，并由`nextPage`值控制：

```py
    def     get_details(page):
        """read 'page' url and append list of queried items to dataSet"""
                nextPage =     True
                pageNo =     1
                    while     (nextPage):
        response = read_url(page +     'page/'     +     str    (pageNo))
            if     response.find(    "ul.pager:has('li.next')"    ):
            nextPage =     True
                else    :
            nextPage =     False

                    quotes = response.find(    '.quote'    )
            print    (    "        \n        Total Quotes found :"    , quotes.    __len__    (),     ' in Page: '    , pageNo)
            for     quote     in     quotes.items():
            title = quote.find(    '[itemprop="text"]:first'    ).text()
            author = quote.find(    '[itemprop="author"]:first'    ).text()
            authorLink = quote.find(    'a[href*="/author/"]:first'    ).attr(    'href'    )
            tags = quote.find(    '.tags [itemprop="keywords"]'    ).attr(    'content'    )

                if     authorLink:
                authorLink =     'http://quotes.toscrape.com'     + authorLink
                linkDetail = read_url(authorLink)
                born_date = linkDetail.find(    '.author-born-date'    ).text()
                born_location = linkDetail.find(    '.author-born-location'    ).text()
                    if     born_location.startswith(    'in'    ):
                    born_location = born_location.replace(    'in '    ,    ''    )

            dataSet.append(    dict    (    zip    (keys,[tags,authorLink,author,born_date,born_location,title[    0    :    50    ]])))

        pageNo +=     1    
```

`:has()`返回与传递给它的选择器匹配的元素。在本例中，我们正在确认`pager`类是否有`next`类的`<li>`元素，即`ul.pager:has('li.next')`。如果表达式为`true`，则另一个页面存在页面链接，`else`终止循环。

使用`items()`对获取的`quotes`进行迭代，得到`title`、`author`、`tags`、`authorLink`。使用`read_url()`函数进一步处理`authorLink`URL，以便分别从`born_date`和`born_location`的`.author-born-date`和`.author-born-location`类中获取与作者相关的特定信息。

我们在前面代码中使用的元素类可以在 Page Source 中找到，如以下屏幕截图所示：

![](assets/f47c0fb5-1f76-4c9e-913f-744b7e77353d.png)

Inner page with author details

`zip()`Python 函数与*键*和引号字段一起使用，作为 Python 字典附加到`dataSet`之后。

上述代码的输出如下所示：

```py
Total Quotes found : 10 in Page: 1
Total Quotes found : 1 in Page: 2
Total Quotes collected: 11

[{'author_name': 'Jane Austen', 'born_location': 'Steventon Rectory, Hampshire, The United Kingdom', 'quote_tags': 'aliteracy,books,classic,humor', 'author_url': 'http://quotes.toscrape.com/author/Jane-Austen', 'quote_title': '“............................... ', 'born_date': 'December 16, 1775'}, 
{'author_name': 'Mark Twain', 'born_location': 'Florida, Missouri, The United States', 'quote_tags': 'books,contentment,friends,friendship,life', 'author_url': 'http://quotes.toscrape.com/author/Mark-Twain', 'quote_title': '“.........................................', 'born_date': 'November 30, 1835'}
,..................................................................................................., 
{'author_name': 'George R.R. Martin', 'born_location': 'Bayonne, New Jersey, The United States', 'quote_tags': 'books,mind', 'author_url': 'http://quotes.toscrape.com/author/George-R-R-Martin', 'quote_title': '“... ...................................', 'born_date': 'September 20, 1948'}]
```

为获得的`dataSet`运行了一个额外的循环，结果是一个字符串，如下所示：

```py
Jane Austen born on December 16, 1775 in Steventon Rectory, Hampshire, The United Kingdom
Mark Twain born on November 30, 1835 in Florida, Missouri, The United States
............................
............................
George R.R. Martin born on September 20, 1948 in Bayonne, New Jersey, The United States
```

# 示例 3–提取 AHL 季后赛结果

在本例中，我们将从**美国曲棍球联盟**（**AHL**）季后赛结果中提取数据，这些数据可从[中获得 http://www.flyershistory.com/cgi-bin/ml-poffs.cgi](http://www.flyershistory.com/cgi-bin/ml-poffs.cgi) ： [](http://www.flyershistory.com/cgi-bin/ml-poffs.cgi) 

![](assets/d25987e0-887b-486b-91a4-ef3741168acb.png)

AHL Playoff results

前面的 URL 包含 AHL 的季后赛结果。本页以表格形式显示有关结果的信息。页面源中显示相关信息的部分如以下屏幕截图所示：

![](assets/18529f0b-9c17-4f1c-a590-e6395d50908c.png)

Page source from http://www.flyershistory.com/cgi-bin/ml-poffs.cgi The preceding screenshot contains the top and bottom part of the tabular information from the source URL and presents two different formats of `<tr>` that are available in the page source. The number of `<td>` that are available in `<tr>` have different, extra information.

分析源格式后，还需要指出，包含所需值的`<td>`没有可用于标识特定表单元格的属性。在这种情况下，我们可以使用 CSS 选择器，即*p seudo 选择器*，例如`td:eq(0)`或`td:eq(1)`，来定位`<td>`或数据单元格的位置。

For more information on CSS selectors, please visit [Chapter 3](03.html), *Using LXML, XPath, and CSS Selectors*, the *Introduction to XPath and CSS selector* section, in the *CSS Selectors* and *Pseudo Selectors* sub-section.

由于本例将使用`pyquery`，因此我们将使用`eq()`方法，该方法接受索引并返回元素。例如，我们可以对选择的 PyQuery 对象`tr`使用`tr.find( 'td' ).eq( 1 ).text()`，搜索索引等于`1`的元素`td`，即`<td>`，并返回该元素的文本。

在此，我们有兴趣收集`keys`中列出的列的数据：

```py
keys = [    'year'    ,    'month'    ,    'day'    ,    'game_date'    ,    'team1'    ,     'team1_score'    ,     'team2'    ,     'team2_score'    ,     'game_status'    ]
```

现在，让我们导入带有`pyquery`和`re`的代码。Regex 将用于分隔从页面来源获取的日期：

```py
    from     pyquery     import     PyQuery     as     pq
    import     re

sourceUrl =     'http://www.flyershistory.com/cgi-bin/ml-poffs.cgi'
    dataSet =     list    ()
keys = [    'year'    ,    'month'    ,    'day'    ,    'game_date'    ,    'team1'    ,     'team1_score'    ,     'team2'    ,     'team2_score'    ,     'game_status'    ]

    def     read_url(url):
        """Read given Url , Returns pyquery object for page content"""
                pageSource = pq(url)
                return     pq(pageSource)

    if     __name__ ==     '__main__'    :
    page = read_url(sourceUrl)                       
```

这里，`read_url()`接受一个参数，即指向页面的链接，并返回页面源或`pageSource`的 PyQuery 对象。PyQuery 自动返回所提供 URL 的页面源。也可以使用其他库获取页面源，如`urllib`、`urllib3`、`requests`和 LXML，并传递来创建 PyQuery 对象：

```py
tableRows = page.find(    "h1:contains('AHL Playoff Results') + table tr"    )
    print    (    "        \n        Total rows found :"    , tableRows.    __len__    ())
```

`tableRows`是一个 PyQuery 对象，用于遍历`<table>`中的`<tr>`，该`<tr>`位于`<h1>`之后。包含`AHL Playoff Results`文本，通过`find()`函数获取。正如我们在下面的输出中所看到的，总共存在`463``<tr>`元素，但从可用`<td>`的数量和实际数据来看，实际获得的记录数量可能更低：

```py
Total rows found : 463
```

让我们再做一些处理。每个`<tr>`或`tr`元素都是`tableRows`的一项，通过`items()`方法遍历，使用其索引并检索其包含的数据，找到准确的`<td>`或`td`：

```py
    for     tr     in     tableRows.items():
    #few <tr> contains single <td> and is omitted using the condition
    team1 = tr.find(    'td'    ).eq(    1    ).text() 

    i    f     team1 !=     ''    :
        game_date = tr.find(    'td'    ).eq(    0    ).text()
        dates = re.search(    r'(.*)-(.*)-(.*)'    ,game_date)
        team1_score = tr.find(    'td'    ).eq(    2    ).text()
        team2 = tr.find(    'td'    ).eq(    4    ).text()
        team2_score = tr.find(    'td'    ).eq(    5    ).text()

            #check Game Status should be either 'W' or 'L'
                    game_status = tr.find(    'td'    ).eq(    6    ).text()
            if not     re.match(    r'[WL]'    ,game_status):
            game_status = tr.find(    'td'    ).eq(    7    ).text()

            #breaking down date in year,month and day
                    year = dates.group(    3    )
        month = dates.group(    2    )
        day = dates.group(    1    )

        #preparing exact year value
            if         len    (year)==    2         and         int    (year)>=    68    :
            year =     '19'    +year
            elif         len    (year)==    2         and         int    (year) <    68    :
            year =     '20'    +year
            else    :
                pass                  
```

到目前为止，已经从目标`<td>`收集了所需的数据，并且在`year`的情况下进行了格式化。Regex 也已应用于代码中，并与`dates`和`game_status`一起使用。最后，收集的对象作为列表附加到`dataSet`：

```py
    #appending individual data list to the dataSet
    dataSet.append([year,month,day,game_date,team1,team1_score,team2,team2_score,game_status])

    print    (    "        \n        Total Game Status, found :"    ,     len    (dataSet))
    print    (dataSet)
```

关于总记录计数和`dataSet`的输出如下：

```py
 Total Game Status, found : 341 
[['1968', 'Apr', '3', '3-Apr-68', 'Buff', '2', 'Que', '4', 'W'],
['1968', 'Apr', '5', '5-Apr-68', 'Buff', '1', 'Que', '3', 'W'], 
['1968', 'Apr', '9', '9-Apr-68', 'Que', '7', 'Buff', '10', 'L'], 
['1968', 'Apr', '10', '10-Apr-68', 'Que', '4', 'Buff', '7', 'L'], 
['1968', 'Apr', '12', '12-Apr-68', 'Buff', '1', 'Que', '3', 'W'],
.................
['2008', 'May', '9', '9-May-2008', 'Phantoms', '3', 'Wilkes-Barre', '1', 'L'], 
['2009', 'Apr', '16', '16-Apr-09', 'Phantoms', '2', 'Hershey', '4', 'L'], 
['2009', 'Apr', '18', '18-Apr-09', 'Phantoms', '2', 'Hershey', '6', 'L'], 
['2009', 'Apr', '22', '22-Apr-09', 'Hershey', '2', 'Phantoms', '3', 'L'], 
['2009', 'Apr', '24', '24-Apr-09', 'Hershey', '0', 'Phantoms', '1', 'L']]
```

# 示例 4–从 sitemap.xml 收集 URL

在本例中，我们将从[中提取`sitemap.xml`文件中为博客找到的 URLhttps://webscraping.com/sitemap.xml](https://webscraping.com/sitemap.xml) 。

在前面的示例中，我们使用了 HTML 内容，但 PyQuery 也可以用于遍历 XML 文件内容。默认情况下，`pyquery`使用基于 LXML 的`xml`解析器，可以在创建 PyQuery 对象时提供该解析器。我们将在文件内容中同时使用`lxml.html`和`xml`。

For more information on `pyquery` and `parser`, please visit the *Exploring pyquery* section of this chapter. For information regarding the site map, please visit [Chapter 1](01.html), *Web Scraping Fundamentals*, the *Data finding techniques* *(seeking data from the web)* section, in the *Sitemaps* subsection.

以下屏幕截图显示了`sitemap.xml`文件中可用的内容：

![](assets/59bb93de-f744-4d15-acf0-5cc90a0e90f1.png)

sitemap.xml file from https://webscraping.com

首先，我们导入`pyquery`并读取文件内容为`xmlFile`

```py
    from     pyquery     import     PyQuery     as     pq

    if     __name__ ==     '__main__'    :
        # reading file
                xmlFile =     open    (    'sitemap.xml'    ,     'r'    ).read()   
```

# 案例 1–使用 HTML 解析器

在这里，我们将使用`lxml.html`解析器解析`xmlFile`，将参数解析器`parser='html'`传递给 PyQuery：

```py
    # creating PyQuery object using parser 'html'
             urlHTML = pq(xmlFile,     parser    =    'html'    )

    print    (    "Children Length: "    ,urlHTML.children().    __len__    ())
    print    (    "First Children: "    ,urlHTML.children().eq(    0    ))
    print    (    "Inner Child/First Children: "    ,urlHTML.children().children().eq(    0    ))
```

使用 PyQuery 的`urlHTML`对象，我们可以检查从数据中获得的计数和子元素，如以下输出所示：

```py
Children Length: 137

First Children: 
<url>
<loc>https://webscraping.com</loc>
</url>

Inner Child/First Children: <loc>https://webscraping.com</loc>
```

如我们所见，`urlHTML.children()`包含查找 URL 所需的元素。我们可以使用`items()`方法处理这些数据，该方法遍历获得的每个元素。让我们创建`dataSet`（Python`list()`），它将附加提取的 URL。

通过使用包含`blog`字符串的选择器，可以使用`urlHTML.children().find( 'loc:contains("blog")' ).items()`执行基于元素的迭代：

```py
dataSet=    list    ()
    for     url     in     urlHTML.children().find(    'loc:contains("blog")'    ).items():
    dataSet.append(url.text())

    print    (    "Length of dataSet: "    ,     len    (dataSet))
    print    (dataSet)
```

最后，我们将收到以下输出：

```py
Length of dataSet: 131

['https://webscraping.com/blog', 'https://webscraping.com/blog/10/', 'https://webscraping.com/blog/11/', 'https://webscraping.com/blog/12/', 'https://webscraping.com/blog/13/', 'https://webscraping.com/blog/2/'
,.................................................................................,
'https://webscraping.com/blog/Reverse-Geocode/', 'https://webscraping.com/blog/Scraping-Flash-based-websites/', 'https://webscraping.com/blog/Scraping-JavaScript-based-web-pages-with-Chickenfoot/', 'https://webscraping.com/blog/category/web2py', 'https://webscraping.com/blog/category/webkit', 'https://webscraping.com/blog/category/website/', 'https://webscraping.com/blog/category/xpath']
```

# 案例 2–使用 XML 解析器

在本例中，我们将使用 PyQuery`urlXML`对象处理 XML 内容，该对象使用`parser='xml'`：

```py
#creating PyQuery object using parser 'xml'
urlXML = pq(xmlFile, parser='xml')

    print    ("Children Length: ",urlXML.children().    __len__    ())
```

前面的代码返回子级计数的长度，即`137`URL 总数：

```py
Children Length: 137 
```

如以下代码所示，第一个和内部子元素返回我们愿意提取的所需 URL 内容：

```py
    print    (    "First Children: "    , urlXML.children().eq(    0    ))
    print    (    "Inner Child/First Children: "    , urlXML.children().children().eq(    0    ))

First Children: 
<url >
<loc>https://webscraping.com</loc>
</url>

Inner Child/First Children: 
<loc >https://webscraping.com</loc>
```

让我们使用类似于*案例 1 中使用的选择器（使用 HTML 解析器*部分）来处理子元素：

```py
dataSet=    list    ()
    for     url     in     urlXML.children().find(    'loc:contains("blog")'    ).items():
    dataSet.append(url.text())

    print    (    "Length of dataSet: "    ,     len    (dataSet))
    print    (dataSet)

```

在这里，我们在`dataSet`中没有收到任何输出，而且选择器似乎不像在*案例 1 中那样工作——使用 HTML 解析器*：

```py
Length of dataSet: 0
[]
```

让我们使用以下代码验证此情况：

```py
    for     url     in     urlXML.children().children().items():
        print    (url)
    break

<loc >https://webscraping.com</loc>
```

我们收到的节点属于[https://www.sitemaps.org/schemas/sitemap/0.9](https://www.sitemaps.org/schemas/sitemap/0.9) 。如果不删除名称空间选择器，它将无法工作。

`remove_namespace()`函数可用于 PyQuery 对象，并对其进行最终输出处理，如下代码所示：

```py
    for     url     in     urlXML.remove_namespaces().children().find(    'loc:contains("blog")'    ).items():
    dataSet.append(url.text())

    print    (    "Length of dataSet: "    ,     len    (dataSet))
    print    (dataSet)
```

我们收到以下输出：

```py
Length of dataSet: 131

['https://webscraping.com/blog', 'https://webscraping.com/blog/10/', 'https://webscraping.com/blog/11/', 'https://webscraping.com/blog/12/', 'https://webscraping.com/blog/13/', 'https://webscraping.com/blog/2/', 'https://webscraping.com/blog/3/', 'https://webscraping.com/blog/4/', 'https://webscraping.com/blog/5/', 'https://webscraping.com/blog/6/', 'https://webscraping.com/blog/7/', 'https://webscraping.com/blog/8/', 
.................................................................
'https://webscraping.com/blog/category/screenshot', 'https://webscraping.com/blog/category/sitescraper', 'https://webscraping.com/blog/category/sqlite', 'https://webscraping.com/blog/category/user-agent', 'https://webscraping.com/blog/category/web2py', 'https://webscraping.com/blog/category/webkit', 'https://webscraping.com/blog/category/website/', 'https://webscraping.com/blog/category/xpath']
```

The PyQuery `remove_namespace()` and `xhtml_to_html()` methods remove the namespaces from XML and XHTML, respectively. Use of these two methods allows us to work with elements that use HTML-related properties.

我们也可以用不同的方法处理相同的内容；也就是说，通过使用正则表达式并根据需要获取输出。让我们继续执行以下代码：

```py
    print    ("URLs using Children: ",urlXML.children().text()) 
    #print    ("URLs using Children: ",urlXML.children().children().text()) 
    #print    ("URLs using Children: ",urlXML.text())
```

PyQuery`children()`object 方法返回所有子节点，`text()`提取文本内容，如下图：

```py
URLs using Children: https://webscraping.com https://webscraping.com/about 
https://webscraping.com/blog .............https://webscraping.com/blog/Converting-UK-Easting-Northing-coordinates/ https://webscraping.com/blog/Crawling-with-threads/ https://webscraping.com/blog/Discount-coupons-for-data-store/ https://webscraping.com/blog/Extracting-article-summaries/ https://webscraping.com/blog/10/ https://webscraping.com/feedback..........
```

如前面的输出所示，来自子节点的所有链接都作为单个字符串返回：

```py
blogXML = re.split(    r'\s'    ,urlXML .children().text())
    print    ("Length of blogXML: ",    len    (blogXML))

#filter(), filters URLs from blogXML that matches string 'blog'
dataSet=     list    (    filter    (    lambda     blogXML:re.findall(    r'blog'    ,blogXML),blogXML))
    print    ("Length of dataSet: ",    len    (dataSet))
    print    ("Blog Urls: ",dataSet)
```

这里，`re.split()`用于拆分接收到的 URL 字符串，其中包含空格字符`\s`。这将返回总共`139`个元素。最后，使用`re.findall()`过滤`blogXML`，在`blogXML`元素中找到`blog`字符串，结果如下：

```py
Length of blogXML: 139
Length of dataSet: 131

Blog Urls: ['https://webscraping.com/blog', 'https://webscraping.com/blog/10/', 'https://webscraping.com/blog/11/', 'https://webscraping.com/blog/12/', 'https://webscraping.com/blog/13/', 'https://webscraping.com/blog/2/', 'https://webscraping.com/blog/3/', 'https://webscraping.com/blog/4/', 'https://webscraping.com/blog/5/', 'https://webscraping.com/blog/6/', 'https://webscraping.com/blog/7/', 'https://webscraping.com/blog/8/',...............................................
'https://webscraping.com/blog/category/web2py', 'https://webscraping.com/blog/category/webkit', 'https://webscraping.com/blog/category/website/', 'https://webscraping.com/blog/category/xpath']
```

在本节中，我们使用了一些刮取技术从文件和网站中提取所需的内容。内容识别和刮取的要求是相当动态的，而且也是基于网站的结构。有了`pyquery`这样的库，我们可以获得并部署必要的工具和技术，以高效的方式进行刮取。

# 总结

`pyquery`似乎在处理 CSS 选择器方面更有效，并提供了许多与 LXML 相关的功能。简单易读的代码总是有需求的，`pyquery`提供了这些特性，用于抓取。在本章中，我们探讨了执行刮片任务时可能遇到的各种情况，并成功地获得了预期的结果。

在下一章中，我们将探索更多与 web 抓取相关的库。

# 进一步阅读

*   PyQuery 完整 API:[https://pyquery.readthedocs.io/en/latest/api.html](https://pyquery.readthedocs.io/en/latest/api.html)
*   pyquery：Python 的类似 jquery 的库：[https://pythonhosted.org/pyquery/](https://pythonhosted.org/pyquery/)
*   CSS 选择器参考：[https://www.w3schools.com/cssref/css_selectors.asp](https://www.w3schools.com/cssref/css_selectors.asp)
*   CSS 伪类和元素：[https://www.w3schools.com/css/css_pseudo_elements.asp](https://www.w3schools.com/css/css_pseudo_elements.asp)
*   CSS 信息：[http://www.css3.info/](http://www.css3.info/) 和[https://developer.mozilla.org/en-US/docs/Web/CSS](https://developer.mozilla.org/en-US/docs/Web/CSS)
*   网站地图：[https://www.sitemaps.org/](https://www.sitemaps.org/)
*   XML*：*[https://www.w3schools.com/xml/](https://www.w3schools.com/xml/) 和[https://www.w3.org/XML/](https://www.w3.org/XML/)*