["```py\n**x = list(func(item) for item in y)**\n**x = list(reversed([func(item) for item in y[::-1]]))**\n\n```", "```py\n**import random**\n**indices= list(range(len(y)))**\n**random.shuffle(indices)**\n**x = [None]*len(y)**\n**for k in indices:**\n **x[k] = func(y[k])**\n\n```", "```py\n99.49.32.197 - - [01/Jun/2012:22:17:54 -0400] \"GET /favicon.ico HTTP/1.1\" 200 894 \"-\" \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.52 Safari/536.5\"\n```", "```py\n**data = path_filter(access_detail_iter(access_iter(local_gzip(filename))))**\n\n```", "```py\n**def local_gzip(pattern):**\n **zip_logs= glob.glob(pattern)**\n **for zip_file in zip_logs:**\n **with gzip.open(zip_file, \"rb\") as log:**\n **yield (line.decode('us-ascii').rstrip() for line in log)**\n\n```", "```py\n **def line_iter(zip_file):**\n **log= gzip.open(zip_file, \"rb\")**\n **return (line.decode('us-ascii').rstrip() for line in log)**\n\n```", "```py\n**map(line_iter, glob.glob(pattern))**\n\n```", "```py\n**format_pat= re.compile(\n    r\"(?P<host>[\\d\\.]+)\\s+\"\n    r\"(?P<identity>\\S+)\\s+\"\n    r\"(?P<user>\\S+)\\s+\"\n    r\"\\[(?P<time>.+?)\\]\\s+\"\n    r'\"(?P<request>.+?)\"\\s+'\n    r\"(?P<status>\\d+)\\s+\"\n    r\"(?P<bytes>\\S+)\\s+\"\n    r'\"(?P<referer>.*?)\"\\s+' # [SIC]\n    r'\"(?P<user_agent>.+?)\"\\s*'\n)** \n\n```", "```py\n**Access = namedtuple('Access', ['host', 'identity', 'user', 'time', 'request', 'status', 'bytes', 'referrer', 'user_agent'])**\n\n```", "```py\n**def access_iter(source_iter):**\n **for log in source_iter:**\n **for line in log:**\n **match= format_pat.match(line)**\n **if match:**\n **yield Access(**match.groupdict())**\n\n```", "```py\n **def access_builder(line):**\n **match= format_pat.match(line)**\n **if match:**\n **return Access(**match.groupdict())**\n\n```", "```py\n **map(access_builder, (line for log in source_iter for line in log))**\n\n```", "```py\n**AccessDetails = namedtuple('AccessDetails', ['access', 'time', 'method', 'url', 'protocol', 'referrer', 'agent'])**\n\n```", "```py\n**AgentDetails= namedtuple('AgentDetails', ['product', 'system', 'platform_details_extensions'])**\n\n```", "```py\n**def access_detail_iter(iterable):**\n **def parse_request(request):**\n **words = request.split()**\n **return words[0], ' '.join(words[1:-1]), words[-1]**\n **def parse_time(ts):**\n **return datetime.datetime.strptime(ts, \"%d/%b/%Y:%H:%M:%S %z\")**\n **agent_pat= re.compile(r\"(?P<product>\\S*?)\\s+\"**\n **r\"\\((?P<system>.*?)\\)\\s*\"**\n **r\"(?P<platform_details_extensions>.*)\")**\n **def parse_agent(user_agent):**\n **agent_match= agent_pat.match(user_agent)**\n **if agent_match:**\n **return AgentDetails(**agent_match.groupdict())**\n\n```", "```py\n **for access in iterable:**\n **try:**\n **meth, uri, protocol = parse_request(access.request)**\n **yield AccessDetails(\n                access= access,\n                time= parse_time(access.time),\n                method= meth,\n                url= urllib.parse.urlparse(uri),\n                protocol= protocol,\n                referrer = urllib.parse.urlparse(access.referer),\n                agent= parse_agent(access.user_agent)** \n **except ValueError as e:**\n **print(e, repr(access))**\n\n```", "```py\n**def access_detail_iter2(iterable):**\n **def access_detail_builder(access):**\n **try:**\n **meth, uri, protocol = parse_request(access.request)**\n **return AccessDetails(access= access,time= parse_time(access.time),method= meth,url= urllib.parse.urlparse(uri),protocol= protocol,referrer = urllib.parse.urlparse(access.referer),agent= parse_agent(access.user_agent))**\n **except ValueError as e:**\n **print(e, repr(access))**\n **return filter(None, map(access_detail_builder, iterable))**\n\n```", "```py\n**def path_filter(access_details_iter):**\n **name_exclude = {'favicon.ico', 'robots.txt', 'humans.txt', 'crossdomain.xml' ,'_images', 'search.html', 'genindex.html', 'searchindex.js', 'modindex.html', 'py-modindex.html',}**\n **ext_exclude = { '.png', '.js', '.css', }**\n **for detail in access_details_iter:**\n **path = detail.url.path.split('/')**\n **if not any(path):**\n **continue**\n **if any(p in name_exclude for p in path):**\n **continue**\n **final= path[-1]**\n **if any(final.endswith(ext) for ext in ext_exclude):**\n **continue**\n **yield detail**\n\n```", "```py\n **def non_empty_path(detail):**\n **path = detail.url.path.split('/')**\n **return any(path)**\n\n```", "```py\n**filter(non_empty_path, access_details_iter)**\n\n```", "```py\n**filter(non_excluded_ext,\n    filter(non_excluded_names,\n        filter(non_empty_path, access_details_iter)))** \n\n```", "```py\n **ne= filter(non_empty_path, access_details_iter)**\n **nx_name= filter(non_excluded_names, ne)**\n **nx_ext= filter(non_excluded_ext, nx_name)**\n **return nx_ext**\n\n```", "```py\n**def book_filter(access_details_iter):**\n **def book_in_path(detail):**\n **path = tuple(l for l in detail.url.path.split('/') if l)**\n **return path[0] == 'book' and len(path) > 1**\n **return filter(book_in_path, access_details_iter)**\n\n```", "```py\n**from collections import Counter**\n**def reduce_book_total(access_details_iter):**\n **counts= Counter()**\n **for detail in access_details_iter:**\n **counts[detail.url.path] += 1**\n **return counts**\n\n```", "```py\n**def analysis(filename):**\n **details= path_filter(access_detail_iter(access_iter(local_gzip(filename))))**\n **books= book_filter(details)**\n **totals= reduce_book_total(books)**\n **return totals**\n\n```", "```py\n **import multiprocessing**\n **with multiprocessing.Pool(4) as workers:**\n **workers.map(analysis, glob.glob(pattern))**\n\n```", "```py\n **import multiprocessing**\n **pattern = \"*.gz\"**\n **combined= Counter()**\n **with multiprocessing.Pool() as workers:**\n **for result in workers.imap_unordered(analysis, glob.glob(pattern)):**\n **combined.update(result)**\n\n```", "```py\n**list(workers.apply(analysis, f) for f in glob.glob(pattern))**\n\n```", "```py\n **import multiprocessing**\n **pattern = \"*.gz\"**\n **combined= Counter()**\n **with multiprocessing.Pool() as workers:**\n **results = workers.map_async(analysis, glob.glob(pattern))**\n **data= results.get()**\n **for c in data:**\n **combined.update(c)**\n\n```", "```py\n **import concurrent.futures**\n **pool_size= 4**\n **pattern = \"*.gz\"**\n **combined= Counter()**\n **with concurrent.futures.ProcessPoolExecutor(max_workers=pool_size) as workers:**\n **for result in workers.map(analysis, glob.glob(pattern)):**\n **combined.update(result)**\n\n```"]