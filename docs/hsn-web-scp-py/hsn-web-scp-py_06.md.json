["```py\npage.find('a').attr('href')    -- (pyquery expression) \n```", "```py\ncssselect('a').get('href')      -- (cssselect expression)\n```", "```py\nC:\\> pip install pyquery\n```", "```py\n>>> from pyquery import PyQuery as pq\n\n>>> print(dir(pq))\n['Fn', '__add__', '__call__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__',  '_filter_only', '_get_root', '_next_all', '_prev_all', '_translator_class', '_traverse','addClass', 'add_class', 'after', 'append', 'appendTo', 'append_to','attr','base_url','before','children', 'clear', 'clone', 'closest', 'contents', 'copy', 'count', 'css','each','empty', 'encoding','end','eq', 'extend', 'filter', 'find','fn','hasClass','has_class','height','hide', 'html', 'index','insert','insertAfter', 'insertBefore', 'insert_after','insert_before', 'is_', 'items', 'length','make_links_absolute',\n'map','next','nextAll','next_all','not_','outerHtml','outer_html','parent','parents', 'pop', 'prepend', 'prependTo', 'prepend_to','prev', 'prevAll', 'prev_all', 'remove', 'removeAttr', 'removeClass', 'remove_attr', 'remove_class','remove_namespaces', 'replaceAll', 'replaceWith', 'replace_all', 'replace_with', 'reverse', 'root','show', siblings','size','sort','text', 'toggleClass', 'toggle_class', 'val', 'width', 'wrap', 'wrapAll','wrap_all','xhtml_to_html']\n```", "```py\n>>> from pyquery import PyQuery as pq\n>>> import requests\n>>> response = requests.get('http://www.example.com').text #content\n\n>>> from urllib.request import urlopen\n>>> response = urlopen('http://www.example.com').read()\n>>> docTree = pq(response)\n```", "```py\n>>> pq(\"https://www.python.org\")\n[<html.no-js>] \n\n>>> site=pq(\"https://www.python.org\")\n>>> print(type(site))\n<class 'pyquery.pyquery.PyQuery'> \n\n>>> pq(\"https://www.samsclub.com\")\n[<html>]\n```", "```py\n>>> doc = pq('http://www.exaple.com', parser = 'xml')  #using parser xml\n\n>>> doc = pq('http://www.exaple.com', parser = 'html') #using parser html\n```", "```py\n>>> doc = pq('<div><p>Testing block</p><p>Second block</p></div>')\n>>> print(type(doc))\n<class 'pyquery.pyquery.PyQuery'>\n\n>>> pagesource = open('test.html','r').read() #reading locally saved HTML\n>>> print(type(pagesource))\n<class 'str'>\n\n>>> page = pq(pagesource)\n>>> print(type(page))\n<class 'pyquery.pyquery.PyQuery'>\n```", "```py\n>>> page('title') #find element <title>\n[<title>]\n\n>>> page.find('title').text() #find element <title> and return text content\n'Welcome to Python.org'\n\n>>> page.find('meta[name=\"description\"]').attr('content')\n'The official home of the Python Programming Language'\n\n>>> page.find('meta[name=\"keywords\"]').attr('content')\n'Python programming language object oriented web free open source software license documentation download community'\n\n>>> buttons = page('a.button').html() #return HTML content for element <a> with class='button'\n>>> buttons\n'>_\\n <span class=\"message\">Launch Interactive Shell</span>\\n ' \n```", "```py\n>>> page('ul.menu') #<ul> element with attribute class='menu'\n[<ul.menu>, <ul.navigation.menu>, <ul.subnav.menu>, <ul.navigation.menu>, <ul.subnav.menu>, <ul.navigation.menu>,..............,<ul.subnav.menu>, <ul.footer-links.navigation.menu.do-not-print>]\n```", "```py\n>>> page('nav:first') #first <nav> element\n[<nav.meta-navigation.container>]\n\n>>> page('a:first') #first <a> element\n[<a>]\n\n>>> page('ul:first') #first <ul> element\n[<ul.menu>]\n\n>>> page('ul:last') #last <ul> element\n[<ul.footer-links.navigation.menu.do-not-print>]\n```", "```py\n>>> page(':header') #header elements found \n[<h1.site-headline>, <h1>, <h1>, <h1>, <h1>, <h1>, <h2.widget-title>, <h2.widget-title>..........,<h2.widget-title>, <h2.widget-title>, <h2.widget-title>]\n\n>>> page(':input') #input elements found\n[<input#id-search-field.search-field>, <button#submit.search-button>]\n\n>>> page(':empty') #empty elements found\n[<meta>, <meta>, <link>, <meta>, <meta>, <meta>, <meta>,<script>, <link>, <link>,........,<img.python-logo>, <span.icon-search>,<span.icon-facebook>, <span.icon-twitter>, <span.icon-freenode>, ...........,<span.icon-feed>, <div.python-logo>, <span#python-status-indicator.python\n-status-indicator-default>, <script>, <script>, <script>]\n\n>>> page(':empty:odd') #empty elements, only Odd ones are listed\n[<meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <script>, <link>, <link>, <link>, <link>, <meta>, .......,<img.python-logo>, <span.icon-google-plus>, <span.icon-twitter>, <span.breaker>, <span.icon-download>, <span.icon-jobs>, <span.icon-calendar>, <span.icon-python>, <div.python-logo>, <script>,<script>]\n```", "```py\n>>> page.find('ul:first').attr('class') #class name of first <ul> element\n'menu'\n\n>>> page.find('a:first').attr('href') #href value of first <a> element\n'#content'\n\n>>> page.find('a:last').attr('href') #href value of last <a> element\n'/psf/sponsorship/sponsors/'\n\n>>> page.find('a:eq(0)').attr('href') #href value of first <a> element using Index!\n'#content'\n\n>>> page.find('a:eq(0)').text() #text from first <a> element\n'Skip to content' \n```", "```py\n>>> page('p:contains(\"Python\")') #return elements <p> with text 'Python\"\n[<p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>]\n\n>>> page('p:contains(\"python.org\")') #return elements <p> with text \"python.org\"\n[<p>, <p>]\n\n#return text from second <p> element containing text \"python.org\"\n>>> page('p:contains(\"python.org\")').eq(1).text() \n'jobs.python.org'\n```", "```py\n#check if class is 'python-logo' >>> page('h1.site-headline:first a img').is_('.python-logo') \n*True*\n\n#check if <img> has class 'python-logo' >>> page('h1.site-headline:first a img').has_class('python-logo') \n*True*\n```", "```py\n#Find <meta> with attribute 'content' containing '..Python.org..' \n#and list the attribute 'name' that satisfies the find()\n\n>>> meta=page.find('meta[content*=\"Python.org\"]')\n>>> [item.attr('name') for item in meta.items() if item.attr('name') is not None]\n['application-name', 'apple-mobile-web-app-title']\n\n#Continuing from code above list value for attribute 'property'\n\n>>> [item.attr('property') for item in meta.items() if item.attr('property') is not None]\n['og:site_name', 'og:title']\n```", "```py\n>>> social = page.find('a:contains(\"Socialize\") + ul.subnav li a') \n>>> [item.text() for item in social.items() if item.text() is not None]\n['Google+', 'Facebook', 'Twitter', 'Chat on IRC']\n\n>>> [item.attr('href') for item in social.items() if item.attr('href') is not None]\n['https://plus.google.com/+Python', 'https://www.facebook.com/pythonlang?fref=ts', 'https://twitter.com/ThePSF', '/community/irc/']\n\n>>> webdevs = page.find('div.applications-widget:first ul.menu li:contains(\"Web Development\") a')\n>>> [item.text() for item in webdevs.items() if item.text() is not None]\n['Django', 'Pyramid', 'Bottle', 'Tornado', 'Flask', 'web2py']\n```", "```py\n>>> eventsList = []\n>>> upcomingevents = page.find('div.event-widget ul.menu li')\n>>> for event in upcomingevents.items():\n ...     time = event.find('time').text()\n ...     url = event.find('a[href*=\"events/python\"]').attr('href')\n ...     title = event.find('a[href*=\"events/python\"]').text()\n ...     eventsList.append([time,title,url])\n ...\n>>> eventsList\n```", "```py\n[['2019-02-19', 'PyCon Namibia 2019', '/events/python-events/790/'], ['2019-02-23', 'PyCascades 2019', '/events/python-events/757/'],\n['2019-02-23', 'PyCon APAC 2019', '/events/python-events/807/'], ['2019-02-23', 'Berlin Python Pizza', '/events/python-events/798/'],\n['2019-03-15', 'Django Girls Rivers 2019 Workshop', '/events/python-user-group/816/']]\n```", "```py\n>>> buttons = page.find('a.button')\n>>> for item in buttons.items():\n...     print(item.text(),' :: ',item.attr('href'))\n...\n\n>_ Launch Interactive Shell  ::  /shell/\nBecome a Member  ::  /users/membership/\nDonate to the PSF  ::  /psf/donations/\n\n>>> buttons = page.find('a.button:odd')\n>>> for item in buttons.items():\n...     print(item.text(),' :: ',item.attr('href'))\n...\n\nBecome a Member  ::  /users/membership/\n\n>>> buttons = page.find('a.button:even')\n>>> for item in buttons.items():\n...     print(item.text(),' :: ',item.attr('href'))\n...\n\n>_ Launch Interactive Shell  ::  /shell/\nDonate to the PSF  ::  /psf/donations/\n```", "```py\nfrom pyquery import PyQuery as pq\nimport requests\ndataSet = list()\n```", "```py\nsourceUrl='https://developer.ibm.com/announcements/' \ndef read_url(url):\n \"\"\"Read given Url , Returns pyquery object for page content\"\"\"\n  pageSource = requests.get(url).content\n return pq(pageSource)\n```", "```py\nif __name__ == '__main__':\n    mainUrl = sourceUrl+\"category/data-science/?fa=date:DESC&fb=\"\n  pageUrls = [sourceUrl+\"category/data-science/page/%(page)s?fa=date:DESC&fb=\" % {'page': page} for page in range(1, 3)]\n\n    for pages in pageUrls:\n        get_details(pages)\n\n    print(\"\\nTotal articles collected: \", len(dataSet))\n    print(dataSet)\n```", "```py\ndef get_details(page):\n    \"\"\"read 'page' url and append list of queried items to dataSet\"\"\"\n  response = read_url(page)\n\n    articles = response.find('.ibm--card > a.ibm--card__block_link')\n    print(\"\\nTotal articles found :\", articles.__len__(), ' in Page: ', page)\n\n    for article in articles.items():\n        link = article.attr('href')\n        articlebody = article.find('div.ibm--card__body')\n\n        adate = articlebody.find('h5 > .ibm--card__date').text()\n        articlebody.find('h5 > .ibm--card__date').remove()\n        atype = articlebody.find('h5').text().strip()\n        title = articlebody.find('h3.ibm--card__title').text().encode('utf-8')\n        excerpt = articlebody.find('p.ibm--card__excerpt').text().encode('utf-8')\n        category = article.find('div.ibm--card__bottom > p.cpt-byline__categories span')\n\n        if link:\n            link = str(link).replace('/announcements/', mainUrl)\n            categories = [span.text for span in category if span.text != '+']\n            dataSet.append([link, atype, adate, title, excerpt,\",\".join(categories)])\n```", "```py\narticlebody.find('h5 > .ibm--card__date').remove())\n```", "```py\nTotal articles found : 8 in Page: https://developer.ibm.com/announcements/category/data-science/page/1?fa=date:DESC&fb=\n\nTotal articles found : 2 in Page: https://developer.ibm.com/announcements/category/data-science/page/2?fa=date:DESC&fb=\n\nTotal articles collected: 10\n\n[['https://developer.ibm.com/announcements/model-mgmt-on-watson-studio-local/', 'Announcement', 'Nov 05, 2018', b'Perform feature engineering and model scoring', b'This code pattern demonstrates how data scientists can leverage IBM Watson Studio Local to automate the building and training of\\xe2\\x80\\xa6', 'Analytics,Apache Spark'], ..........................., ['https://developer.ibm.com/announcements/algorithm-that-gives-you-answer-to-any-particular-question-based-on-mining-documents/', 'Announcement', 'Sep 17, 2018', b'Query a knowledge base to get insights about data', b'Learn a strategy to query a knowledge graph with a question and find the right answer.', 'Artificial Intelligence,Data Science'], ['https://developer.ibm.com/announcements/build-a-domain-specific-knowledge-graph-from-given-set-of-documents/', 'Announcement', 'Sep 14, 2018', b'Walk through the process of building a knowledge base by mining information stored in the documents', b'Take a look at all of the aspects of building a domain-specific knowledge graph.', 'Artificial Intelligence,Data Science']]\n```", "```py\nfrom pyquery import PyQuery as pq\nsourceUrl = 'http://quotes.toscrape.com/tag/books/' dataSet = list()\nkeys = ['quote_tags','author_url','author_name','born_date','born_location','quote_title']\n\ndef read_url(url):\n    \"\"\"Read given Url , Returns pyquery object for page content\"\"\"\n  pageSource = pq(url)\n    return pq(pageSource)\n```", "```py\nif __name__ == '__main__':\n    get_details(sourceUrl)\n\n    print(\"\\nTotal Quotes collected: \", len(dataSet))\n    print(dataSet)\n\n    for info in dataSet:\n        print(info['author_name'],' born on ',info['born_date'], ' in ',info['born_location'])\n```", "```py\ndef get_details(page):\n    \"\"\"read 'page' url and append list of queried items to dataSet\"\"\"\n  nextPage = True\n  pageNo = 1\n  while (nextPage):\n        response = read_url(page + 'page/' + str(pageNo))\n        if response.find(\"ul.pager:has('li.next')\"):\n            nextPage = True\n else:\n            nextPage = False    quotes = response.find('.quote')\n        print(\"\\nTotal Quotes found :\", quotes.__len__(), ' in Page: ', pageNo)\n        for quote in quotes.items():\n            title = quote.find('[itemprop=\"text\"]:first').text()\n            author = quote.find('[itemprop=\"author\"]:first').text()\n            authorLink = quote.find('a[href*=\"/author/\"]:first').attr('href')\n            tags = quote.find('.tags [itemprop=\"keywords\"]').attr('content')\n\n            if authorLink:\n                authorLink = 'http://quotes.toscrape.com' + authorLink\n                linkDetail = read_url(authorLink)\n                born_date = linkDetail.find('.author-born-date').text()\n                born_location = linkDetail.find('.author-born-location').text()\n                if born_location.startswith('in'):\n                    born_location = born_location.replace('in ','')\n\n            dataSet.append(dict(zip(keys,[tags,authorLink,author,born_date,born_location,title[0:50]])))\n\n        pageNo += 1\n```", "```py\nTotal Quotes found : 10 in Page: 1\nTotal Quotes found : 1 in Page: 2\nTotal Quotes collected: 11\n\n[{'author_name': 'Jane Austen', 'born_location': 'Steventon Rectory, Hampshire, The United Kingdom', 'quote_tags': 'aliteracy,books,classic,humor', 'author_url': 'http://quotes.toscrape.com/author/Jane-Austen', 'quote_title': '\u201c............................... ', 'born_date': 'December 16, 1775'}, \n{'author_name': 'Mark Twain', 'born_location': 'Florida, Missouri, The United States', 'quote_tags': 'books,contentment,friends,friendship,life', 'author_url': 'http://quotes.toscrape.com/author/Mark-Twain', 'quote_title': '\u201c.........................................', 'born_date': 'November 30, 1835'}\n,..................................................................................................., \n{'author_name': 'George R.R. Martin', 'born_location': 'Bayonne, New Jersey, The United States', 'quote_tags': 'books,mind', 'author_url': 'http://quotes.toscrape.com/author/George-R-R-Martin', 'quote_title': '\u201c... ...................................', 'born_date': 'September 20, 1948'}]\n```", "```py\nJane Austen born on December 16, 1775 in Steventon Rectory, Hampshire, The United Kingdom\nMark Twain born on November 30, 1835 in Florida, Missouri, The United States\n............................\n............................\nGeorge R.R. Martin born on September 20, 1948 in Bayonne, New Jersey, The United States\n```", "```py\nkeys = ['year','month','day','game_date','team1', 'team1_score', 'team2', 'team2_score', 'game_status']\n```", "```py\nfrom pyquery import PyQuery as pq\nimport re\n\nsourceUrl = 'http://www.flyershistory.com/cgi-bin/ml-poffs.cgi' dataSet = list()\nkeys = ['year','month','day','game_date','team1', 'team1_score', 'team2', 'team2_score', 'game_status']\n\ndef read_url(url):\n    \"\"\"Read given Url , Returns pyquery object for page content\"\"\"\n  pageSource = pq(url)\n  return pq(pageSource)\n\nif __name__ == '__main__':\n    page = read_url(sourceUrl)  \n```", "```py\ntableRows = page.find(\"h1:contains('AHL Playoff Results') + table tr\")\nprint(\"\\nTotal rows found :\", tableRows.__len__())\n```", "```py\nTotal rows found : 463\n```", "```py\nfor tr in tableRows.items():\n    #few <tr> contains single <td> and is omitted using the condition\n    team1 = tr.find('td').eq(1).text() \n\n    if team1 != '':\n        game_date = tr.find('td').eq(0).text()\n        dates = re.search(r'(.*)-(.*)-(.*)',game_date)\n        team1_score = tr.find('td').eq(2).text()\n        team2 = tr.find('td').eq(4).text()\n        team2_score = tr.find('td').eq(5).text()\n\n        #check Game Status should be either 'W' or 'L'\n  game_status = tr.find('td').eq(6).text()\n        if not re.match(r'[WL]',game_status):\n            game_status = tr.find('td').eq(7).text()\n\n        #breaking down date in year,month and day\n  year = dates.group(3)\n        month = dates.group(2)\n        day = dates.group(1)\n\n        #preparing exact year value\n        if len(year)==2 and int(year)>=68:\n            year = '19'+year\n        elif len(year)==2 and int(year) <68:\n            year = '20'+year\n        else:\n            pass  \n```", "```py\n#appending individual data list to the dataSet dataSet.append([year,month,day,game_date,team1,team1_score,team2,team2_score,game_status])\n\nprint(\"\\nTotal Game Status, found :\", len(dataSet))\nprint(dataSet)\n```", "```py\nTotal Game Status, found : 341 \n[['1968', 'Apr', '3', '3-Apr-68', 'Buff', '2', 'Que', '4', 'W'],\n['1968', 'Apr', '5', '5-Apr-68', 'Buff', '1', 'Que', '3', 'W'], \n['1968', 'Apr', '9', '9-Apr-68', 'Que', '7', 'Buff', '10', 'L'], \n['1968', 'Apr', '10', '10-Apr-68', 'Que', '4', 'Buff', '7', 'L'], \n['1968', 'Apr', '12', '12-Apr-68', 'Buff', '1', 'Que', '3', 'W'],\n.................\n['2008', 'May', '9', '9-May-2008', 'Phantoms', '3', 'Wilkes-Barre', '1', 'L'], \n['2009', 'Apr', '16', '16-Apr-09', 'Phantoms', '2', 'Hershey', '4', 'L'], \n['2009', 'Apr', '18', '18-Apr-09', 'Phantoms', '2', 'Hershey', '6', 'L'], \n['2009', 'Apr', '22', '22-Apr-09', 'Hershey', '2', 'Phantoms', '3', 'L'], \n['2009', 'Apr', '24', '24-Apr-09', 'Hershey', '0', 'Phantoms', '1', 'L']]\n```", "```py\nfrom pyquery import PyQuery as pq\n\nif __name__ == '__main__':\n    # reading file\n  xmlFile = open('sitemap.xml', 'r').read()   \n```", "```py\n# creating PyQuery object using parser 'html'\n  urlHTML = pq(xmlFile, parser='html')\n\nprint(\"Children Length: \",urlHTML.children().__len__())\nprint(\"First Children: \",urlHTML.children().eq(0))\nprint(\"Inner Child/First Children: \",urlHTML.children().children().eq(0))\n```", "```py\nChildren Length: 137\n\nFirst Children: \n<url>\n<loc>https://webscraping.com</loc>\n</url>\n\nInner Child/First Children: <loc>https://webscraping.com</loc>\n```", "```py\ndataSet=list()\nfor url in urlHTML.children().find('loc:contains(\"blog\")').items():\n    dataSet.append(url.text())\n\nprint(\"Length of dataSet: \", len(dataSet))\nprint(dataSet)\n```", "```py\nLength of dataSet: 131\n\n['https://webscraping.com/blog', 'https://webscraping.com/blog/10/', 'https://webscraping.com/blog/11/', 'https://webscraping.com/blog/12/', 'https://webscraping.com/blog/13/', 'https://webscraping.com/blog/2/'\n,.................................................................................,\n'https://webscraping.com/blog/Reverse-Geocode/', 'https://webscraping.com/blog/Scraping-Flash-based-websites/', 'https://webscraping.com/blog/Scraping-JavaScript-based-web-pages-with-Chickenfoot/', 'https://webscraping.com/blog/category/web2py', 'https://webscraping.com/blog/category/webkit', 'https://webscraping.com/blog/category/website/', 'https://webscraping.com/blog/category/xpath']\n```", "```py\n#creating PyQuery object using parser 'xml'\nurlXML = pq(xmlFile, parser='xml')\n\nprint(\"Children Length: \",urlXML.children().__len__())\n```", "```py\nChildren Length: 137 \n```", "```py\nprint(\"First Children: \", urlXML.children().eq(0))\nprint(\"Inner Child/First Children: \", urlXML.children().children().eq(0))\n\nFirst Children: \n<url >\n<loc>https://webscraping.com</loc>\n</url>\n\nInner Child/First Children: \n<loc >https://webscraping.com</loc>\n```", "```py\ndataSet=list()\nfor url in urlXML.children().find('loc:contains(\"blog\")').items():\n    dataSet.append(url.text())\n\nprint(\"Length of dataSet: \", len(dataSet))\nprint(dataSet)\n\n```", "```py\nLength of dataSet: 0\n[]\n```", "```py\nfor url in urlXML.children().children().items():\n    print(url)\n    break\n\n<loc >https://webscraping.com</loc>\n```", "```py\nfor url in urlXML.remove_namespaces().children().find('loc:contains(\"blog\")').items():\n    dataSet.append(url.text())\n\nprint(\"Length of dataSet: \", len(dataSet))\nprint(dataSet)\n```", "```py\nLength of dataSet: 131\n\n['https://webscraping.com/blog', 'https://webscraping.com/blog/10/', 'https://webscraping.com/blog/11/', 'https://webscraping.com/blog/12/', 'https://webscraping.com/blog/13/', 'https://webscraping.com/blog/2/', 'https://webscraping.com/blog/3/', 'https://webscraping.com/blog/4/', 'https://webscraping.com/blog/5/', 'https://webscraping.com/blog/6/', 'https://webscraping.com/blog/7/', 'https://webscraping.com/blog/8/', \n.................................................................\n'https://webscraping.com/blog/category/screenshot', 'https://webscraping.com/blog/category/sitescraper', 'https://webscraping.com/blog/category/sqlite', 'https://webscraping.com/blog/category/user-agent', 'https://webscraping.com/blog/category/web2py', 'https://webscraping.com/blog/category/webkit', 'https://webscraping.com/blog/category/website/', 'https://webscraping.com/blog/category/xpath']\n```", "```py\nprint(\"URLs using Children: \",urlXML.children().text()) \n#print(\"URLs using Children: \",urlXML.children().children().text()) \n#print(\"URLs using Children: \",urlXML.text())\n```", "```py\nURLs using Children: https://webscraping.com https://webscraping.com/about \nhttps://webscraping.com/blog .............https://webscraping.com/blog/Converting-UK-Easting-Northing-coordinates/ https://webscraping.com/blog/Crawling-with-threads/ https://webscraping.com/blog/Discount-coupons-for-data-store/ https://webscraping.com/blog/Extracting-article-summaries/ https://webscraping.com/blog/10/ https://webscraping.com/feedback..........\n```", "```py\nblogXML = re.split(r'\\s',urlXML .children().text())\nprint(\"Length of blogXML: \",len(blogXML))\n\n#filter(), filters URLs from blogXML that matches string 'blog'\ndataSet= list(filter(lambda blogXML:re.findall(r'blog',blogXML),blogXML))\nprint(\"Length of dataSet: \",len(dataSet))\nprint(\"Blog Urls: \",dataSet)\n```", "```py\nLength of blogXML: 139\nLength of dataSet: 131\n\nBlog Urls: ['https://webscraping.com/blog', 'https://webscraping.com/blog/10/', 'https://webscraping.com/blog/11/', 'https://webscraping.com/blog/12/', 'https://webscraping.com/blog/13/', 'https://webscraping.com/blog/2/', 'https://webscraping.com/blog/3/', 'https://webscraping.com/blog/4/', 'https://webscraping.com/blog/5/', 'https://webscraping.com/blog/6/', 'https://webscraping.com/blog/7/', 'https://webscraping.com/blog/8/',...............................................\n'https://webscraping.com/blog/category/web2py', 'https://webscraping.com/blog/category/webkit', 'https://webscraping.com/blog/category/website/', 'https://webscraping.com/blog/category/xpath']\n```"]