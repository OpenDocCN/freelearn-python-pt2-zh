["```py\n>>> import re\n>>> print(dir(re)) #listing features from re\n```", "```py\n['A', 'ASCII', 'DEBUG', 'DOTALL', 'I', 'IGNORECASE', 'L', 'LOCALE', 'M', 'MULTILINE', 'S', 'Scanner', 'T', 'TEMPLATE', 'U', 'UNICODE', 'VERBOSE', 'X', '_MAXCACHE', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__versio n__', '_alphanum_bytes', '_alphanum_str', '_cache', '_cache_repl', '_compile', '_compile_repl', '_expand', '_locale', '_pattern_type', '_pickle', '_subx', 'compile', 'copyreg', 'error', 'escape', 'findall', 'finditer', 'fullmatch', 'match', 'purge', 'search', 'split', 'sre_compile', 'sre_parse', 'sub', 'subn', 'sys', 'template']\n```", "```py\n>>> sentence = \"\"\"Brief information about Jobs in Python. Programming and Scripting experience in some language (such as Python R, MATLAB, SAS, Mathematica, Java, C, C++, VB, JavaScript or FORTRAN) is expected. Participants should be comfortable with basic programming concepts like variables, loops, and functions.\"\"\"\n```", "```py\n>>> splitSentence = sentence.split() #split sentence or re.split(r'\\s',sentence) >>> print(\"Length of Sentence: \",len(sentence), '& splitSentence: ',len(splitSentence))\nLength of Sentence: 297 & splitSentence: 42 >>> print(splitSentence) #List of words obtained using split() \n['Brief', 'information', 'about', 'Jobs', 'in', 'Python.', 'Programming', 'and', 'Scripting', 'experience', 'in', 'some', 'language', '(such', 'as', 'Python', 'R,', 'MATLAB,', 'SAS,', 'Mathematica,', 'Java,', 'C,', 'C++,', 'VB,', 'JavaScript', 'or', 'FORTRAN)', 'is', 'expected.', 'Participants', 'should', 'be', 'comfortable', 'with', 'basic', 'programming', 'concepts', 'like', 'variables,', 'loops,', 'and', 'functions.']\n```", "```py\n>>> matches = re.findall(r\"([A-Z+]+)\\,\",sentence) #finding pattern with [A-Z+] and comma behind >>> print(\"Findall found total \",len(matches),\" Matches >> \",matches) **Findall found total  6  Matches >>  ['R', 'MATLAB', 'SAS', 'C', 'C++', 'VB']** >>> matches = re.findall(r\"([A-Z]+)\\,\",sentence) #finding pattern with [A-Z] and comma behind >>> print(\"Findall found total \",len(matches),\" Matches >> \",matches) Findall found total 5 Matches >> ['R', 'MATLAB', 'SAS', 'C', 'VB']\n```", "```py\n>>> matches = re.findall(r\"\\s*([\\sorA-Z+]+)\\)\",sentence) #r'\\s*([A-Z]+)\\)' matches 'FORTRAN' \n>>> print(\"Findall found total \",len(matches),\" Matches >> \",matches)\n\nFindall found total  1  Matches >>  ['or FORTRAN']\n\n>>> fortran = matches[0] # 'or FORTRAN'\n>>> if re.match(r'or',fortran): \n fortran = re.sub(r'or\\s*','',fortran) #substitute 'or ' with empty string >>> print(fortran)\n\nFORTRAN\n\n>>> if re.search(r'^F.*N$',fortran):  #using beginning and end of line searching pattern \n print(\"True\")\n True\n```", "```py\n>>> matches  = re.findall(r'\\s(MAT.*?)\\,',sentence,flags=re.IGNORECASE)\n>>> print(\"(MAT.*?)\\,: \",matches)  #r'(?i)\\s(MAT.*?)\\,' can also be used\n (MAT.*?)\\,: ['MATLAB', 'Mathematica']   >>> matches = re.findall(r'\\s(MAT.*?)\\,',sentence) #findall with 'MAT' case-sensitive\n>>> print(\"(MAT.*?)\\,: \",matches)\n (MAT.*?)\\,: ['MATLAB']   >>> matches = re.findall(r'\\s(C.*?)\\,',sentence)\n>>> print(\"\\s(C.*?)\\,: \",matches)\n \\s(C.*?)\\,: ['C', 'C++']\n```", "```py\n>>> matchesOne = re.split(r\"\\W+\",sentence)  #split by word, \\w (word characters, \\W - nonword) >>> print(\"Regular Split '\\W+' found total: \",len(matchesOne ),\"\\n\",matchesOne)  Regular Split '\\W+' found total: 43 \n['Brief', 'information', 'about', 'Jobs', 'in', 'Python', 'Programming', 'and', 'Scripting', 'experience', 'in', 'some', 'language', 'such', 'as', 'Python', 'R', 'MATLAB', 'SAS', 'Mathematica', 'Java', 'C', 'C', 'VB', 'JavaScript', 'or', 'FORTRAN', 'is', 'expected', 'Participants', 'should', 'be', 'comfortable', 'with', 'basic', 'programming', 'concepts', 'like', 'variables', 'loops', 'and', 'functions', ''] >>> matchesTwo = re.split(r\"\\s\",sentence) #split by space\n>>> print(\"Regular Split '\\s' found total: \",len(matchesTwo),\"\\n\", matchesTwo) **Regular Split '\\s' found total: 42** \n**['Brief', 'information', 'about', 'Jobs', 'in', 'Python.', 'Programming', 'and', 'Scripting', 'experience', 'in', 'some', 'language', '(such', 'as', 'Python', 'R,', 'MATLAB,', 'SAS,', 'Mathematica,', 'Java,', 'C,', 'C++,', 'VB,', 'JavaScript', 'or', 'FORTRAN)', 'is', 'expected.', 'Participants', 'should', 'be', 'comfortable', 'with', 'basic', 'programming', 'concepts', 'like', 'variables,', 'loops,', 'and', 'functions.']**\n```", "```py\n>>> timeDate= '''<time datetime=\"2019-02-11T18:00:00+00:00\"></time> <time datetime=\"2018-02-11T13:59:00+00:00\"></time> <time datetime=\"2019-02-06T13:44:00.000002+00:00\"></time> <time datetime=\"2019-02-05T17:39:00.000001+00:00\"></time> <time datetime=\"2019-02-04T12:53:00+00:00\"></time>''' >>> pattern = r'(20\\d+)([-]+)(0[1-9]|1[012])([-]+)(0[1-9]|[12][0-9]|3[01])' >>> recompiled = re.compile(pattern)  # <class '_sre.SRE_Pattern'>\n>>> dateMatches = recompiled.search(timeDate)\n```", "```py\n>>> print(\"Group : \",dateMatches.group()) \nGroup : 2019-02-11\n >>> print(\"Groups : \",dateMatches.groups())\nGroups : ('2019', '-', '02', '-', '11')\n >>> print(\"Group 1 : \",dateMatches.group(1))\nGroup 1 : 2019\n >>> print(\"Group 5 : \",dateMatches.group(5))\nGroup 5 : 11\n```", "```py\n>>> for match in re.finditer(pattern, timeDate): # <class '_sre.SRE_Match'>\n #for match in re.finditer(recompiled, timeDate):\n s = match.start()\n e = match.end()\n l = match.lastindex\n g = match.groups()\n\n print('Found {} at {}:{}, groups{} lastindex:{}'.format(timeDate[s:e], s, e,g,l))\n\nFound 2019-02-11 at 16:26, groups('2019', '-', '02', '-', '11') lastindex:5\nFound 2018-02-11 at 67:77, groups('2018', '-', '02', '-', '11') lastindex:5\nFound 2019-02-06 at 118:128, groups('2019', '-', '02', '-', '06') lastindex:5\nFound 2019-02-05 at 176:186, groups('2019', '-', '02', '-', '05') lastindex:5\nFound 2019-02-04 at 234:244, groups('2019', '-', '02', '-', '04') lastindex:5\n```", "```py\n>>> pDate = r'(?P<year>[0-9]{4})(?P<sep>[-])(?P<month>0[1-9]|1[012])-(?P<day>0[1-9]|[12][0-9]|3[01])' >>> recompiled = re.compile(pDate) #compiles the pattern >>> for match in re.finditer(recompiled,timeDate): #apply pattern on timeDate\n s = match.start()\n e = match.end()\n l = match.lastindex\n\n print(\"Group ALL or 0: \",match.groups(0)) #or match.groups() that is all\n print(\"Group Year: \",match.group('year')) #return year\n print(\"Group Month: \",match.group('month')) #return month\n print(\"Group Day: \",match.group('day')) #return day\n\n print(\"Group Delimiter: \",match.group('sep')) #return seperator\n print('Found {} at {}:{}, lastindex: {}'.format(timeDate[s:e], s, e,l))\n\n print('year :',match.groupdict()['year']) #accessing groupdict()\n print('day :',match.groupdict()['day'])\n\n print('lastgroup :',match.lastgroup) #lastgroup name\n```", "```py\nGroup ALL or 0: ('2019', '-', '02', '11')\nGroup Year: 2019\nGroup Month: 02\nGroup Day: 11\nGroup Delimiter: -\nFound 2019-02-11 at 16:26, lastindex: 4\nyear : 2019\nday : 11\nlastgroup : day\n```", "```py\n>>> pTime = r'(?P<hour>[0-9]{2})(?P<sep>[:])(?P<min>[0-9]{2}):(?P<sec_mil>[0-9.:+]+)'\n>>> recompiled = re.compile(pTime)\n\n>>> for match in re.finditer(recompiled,timeDate):\n print(\"Group String: \",match.group()) #groups\n print(\"Group ALL or 0: \",match.groups())\n\n print(\"Group Span: \",match.span()) #using span()\n print(\"Group Span 1: \",match.span(1))\n print(\"Group Span 4: \",match.span(4))\n\n print('hour :',match.groupdict()['hour']) #accessing groupdict()\n print('minute :',match.groupdict()['min'])\n print('second :',match.groupdict()['sec_mil'])\n\n print('lastgroup :',match.lastgroup) #lastgroup name\n```", "```py\nGroup String: 12:53:00+00:00\nGroup ALL or 0: ('12', ':', '53', '00+00:00')\nGroup Span: (245, 259)\nGroup Span 1: (245, 247)\nGroup Span 4: (251, 259)\nhour : 12\nminute : 53\nsecond : 00+00:00\nlastgroup : sec_mil\n```", "```py\n<html>\n<head>\n   <title>Welcome to Web Scraping: Example</title>\n   <style type=\"text/css\">\n        ....\n   </style>\n</head>\n<body>\n    <h1 style=\"color:orange;\">Welcome to Web Scraping</h1>\n     Links:\n    <a href=\"https://www.google.com\" style=\"color:red;\">Google</a>   <a class=\"classOne\" href=\"https://www.yahoo.com\">Yahoo</a>   <a id=\"idOne\" href=\"https://www.wikipedia.org\" style=\"color:blue;\">Wikipedia</a>\n    <div>\n        <p id=\"mainContent\" class=\"content\">\n            <i>Paragraph contents</i>\n            <img src=\"mylogo.png\" id=\"pageLogo\" class=\"logo\"/>\n        </p>\n        <p class=\"content\" id=\"subContent\">\n            <i style=\"color:red\">Sub paragraph content</i>\n            <h1 itemprop=\"subheading\">Sub heading Content!</h1>\n        </p>\n    </div>\n</body>\n</html>\n```", "```py\nimport re\nfrom bs4 import BeautifulSoup\n\ndef read_file():\n   ''' Read and return content from file (.html). '''  content = open(\"regexHTML.html\", \"r\")\n    pageSource = content.read()\n    return pageSource\n\ndef applyPattern(pattern):\n'''Applies regex pattern provided to Source and prints count and contents'''\n    elements = re.findall(pattern, page) #apply pattern to source\n    print(\"Pattern r'{}' ,Found total: {}\".format(pattern,len(elements)))\n    print(elements) #print all found tags\n    return   if __name__ == \"__main__\":\n    page = read_file() #read HTML file \n```", "```py\nsoup = BeautifulSoup(page, 'lxml')\nprint([element.name for element in soup.find_all()])\n['html', 'head', 'title', 'style', 'body', 'h1', 'a', 'a', 'a', 'div', 'p', 'i', 'img', 'p', 'i', 'h1']\n```", "```py\napplyPattern(r'<(\\w+)>') #Finding Elements without attributes \nPattern r'<(\\w+)>' ,Found total: 6\n['html', 'head', 'title', 'body', 'div', 'i']\n```", "```py\napplyPattern(r'<(\\w+)\\s') #Finding Elements with attributes \nPattern r'<(\\w+)\\s' ,Found total: 10\n['style', 'h1', 'a', 'a', 'a', 'p', 'img', 'p', 'i', 'h1']\n```", "```py\napplyPattern(r'<(\\w+)\\s?') #Finding all HTML element\n\nPattern r'<(\\w+)\\s?' ,Found total: 16\n['html', 'head', 'title', 'style', 'body', 'h1', 'a', 'a', 'a', 'div', 'p', 'i', 'img', 'p', 'i', 'h1']\n```", "```py\napplyPattern(r'<\\w+\\s+(.*?)=') #Finding attributes name Pattern r'<\\w+\\s+(.*?)=' ,Found total: 10\n['type', 'style', 'href', 'class', 'id', 'id', 'src', 'class', 'style', 'itemprop']\n```", "```py\napplyPattern(r'(\\w+)=') #Finding names of all attributes Pattern r'(\\w+)=' ,Found total: 18\n['type', 'style', 'href', 'style', 'class', 'href', 'id', 'href', 'style', 'id', 'class', 'src', 'id', 'class', 'class', 'id', 'style', 'itemprop']\n```", "```py\napplyPattern(r'=\\\"(\\w+)\\\"')\n\nPattern r'=\\\"(\\w+)\\\"' ,Found total: 9\n['classOne', 'idOne', 'mainContent', 'content', 'pageLogo', 'logo', 'content', 'subContent', 'subheading']\n```", "```py\napplyPattern(r'=\\\"([\\w\\S]+)\\\"')\n\nPattern r'=\\\"([\\w\\S]+)\\\"' ,Found total: 18\n['text/css', 'color:orange;', 'https://www.google.com', 'color:red;', 'classOne', 'https://www.yahoo.com', 'idOne', 'https://www.wikipedia.org', 'color:blue;', 'mainContent', 'content', 'mylogo.png', 'pageLogo', 'logo', 'content', 'subContent', 'color:red', 'subheading']\n```", "```py\napplyPattern(r'\\>(.*)\\<')\nPattern r'\\>(.*)\\<' ,Found total: 8\n['Welcome to Web Scraping: Example', 'Welcome to Web Scraping', 'Google', 'Yahoo', 'Wikipedia', 'Paragraph contents', 'Sub paragraph content', 'Sub heading Content!']  \n```", "```py\nimport re\nimport requests\n def read_url(url):\n'''\nHandles URL Request and Response\nLoads the URL provided using requests and returns the text of page source\n'''\n  pageSource = requests.get(url).text\n    return pageSource\n\nif __name__ == \"__main__\":\n```", "```py\ndataSet=list() #collecting data extracted\nsourceUrl = 'http://godfreysfeed.com/dealersandlocations.php' page = read_url(sourceUrl) #load sourceUrl and return the page source\n```", "```py\n#Defining pattern matching latitude and longitude as found in page.\npLatLng= r'var latLng = new google.maps.LatLng\\((?P<lat>.*)\\,\\s*(?P<lng>.*)\\)\\;'\n\n#applying pattern to page source latlngs = re.findall(pLatLng,page) \nprint(\"Findall found total *LatLngs:* \", len(latlngs))\n\n#Print coordinates found\nprint(latlngs)\n```", "```py\nFindall found total LatLngs: 55 \n[('33.2509855','-84.2633946'),('31.0426107','-84.8821949'),('34.8761989','-83.9582412'),('32.43158','-81.749293'),('33.8192864','-83.4387722'),('34.2959968','-83.0062267'),\n('32.6537561','-83.7596295'),('31.462497','-82.5866503'),('33.7340136','-82.7472304')\n,................................................................., \n('32.5444125','-82.8945945'),('32.7302168','-82.7117232'),('34.0082425','-81.7729772'),\n('34.6639864', '-82.5126743'),('31.525261','-83.06603'),('34.2068698','-83.4689814'),\n('32.9765932','-84.98978'),('34.0412765','-83.2001394'),('33.3066615','-83.6976187'), \n('31.3441482','-83.3002373'),('30.02116','-82.329495'),('34.58403','-83.760829')]\n```", "```py\n#Defining pattern to find dealer from page.\npDealers = r'infoWindowContent = infoWindowContent\\+\\s*\\\"(.*?)\\\"\\;'\n\n#applying dealers pattern to page source dealers = re.findall(pDealers, page)\nprint(\"Findall found total Address: \", len(dealers))\n\n#Print dealers information found\nprint(dealers)\n```", "```py\nFindall found total Address: 55\n\n[\"<strong><span style='color:#e5011c;'>Akins Feed & Seed</span></strong><br><strong>206 N Hill Street </strong><br><strong>Griffin, GA</strong><br><strong>30223</strong><br><br>\", \"<strong><span style='color:#e5011c;'>Alf&apos;s Farm and Garden</span></strong><br><strong>101 East 1st Street</strong><br><strong>Donalsonville, GA</strong><br><strong>39845</strong><br><br>\", \"<strong><span style='color:#e5011c;'>American Cowboy Shop</span></strong><br><strong>513 D Murphy Hwy</strong><br><strong>Blairsville, GA</strong><br><strong>30512</strong><br><br>\",................................... ....................................,\"<strong><span style='color:#e5011c;'>White Co. Farmers Exchange </span></strong><br><strong>951 S Main St</strong><br><strong>Cleveland, GA</strong><br><strong>30528 </strong><br><br>\"]\n```", "```py\nd=0 #maintaining loop counter for dealer in dealers:\n    dealerInfo = re.split(r'<br>',re.sub(r'<br><br>','',dealer))\n\n    #extract individual item from dealerInfo\n    name = re.findall(r'\\'>(.*?)</span',dealerInfo[0])[0]\n    address = re.findall(r'>(.*)<',dealerInfo[1])[0]\n    city = re.findall(r'>(.*),\\s*(.*)<',dealerInfo[2])[0][0]\n    state = re.findall(r'>(.*),\\s*(.*)<',dealerInfo[2])[0][1]\n    zip = re.findall(r'>(.*)<',dealerInfo[3])[0]\n    lat = latlngs[d][0]\n    lng = latlngs[d][1]\n    d+=1\n\n    #appending items to dataset\n  dataSet.append([name,address,city,state,zip,lat,lng])\n print(dataSet)  #[[name,address, city, state, zip, lat,lng],]\n```", "```py\n[['Akins Feed & Seed', '206 N Hill Street', 'Griffin', 'GA', '30223', '33.2509855', '-84.2633946'], ['Alf&apos;s Farm and Garden', '101 East 1st Street', 'Donalsonville', 'GA', '39845', '31.0426107', '-84.8821949'],...................................., \n['Twisted Fitterz', '10329 Nashville Enigma Rd', 'Alapaha', 'GA', '31622', '31.3441482', '-83.3002373'], \n['Westside Feed II', '230 SE 7th Avenue', 'Lake Butler', 'FL', '32054', '30.02116', '-82.329495'],\n['White Co. Farmers Exchange', '951 S Main St', 'Cleveland', 'GA', '30528', '34.58403', '-83.760829']]\n```", "```py\nimport re\n\nfilename = 'sitemap.xml' dataSetBlog = [] # collect Blog title information from URLs except 'category' dataSetBlogURL = [] # collects Blog URLs dataSetCategory = [] # collect Category title dataSetCategoryURL = [] # collect Category URLs   page = open(filename, 'r').read()\n```", "```py\n#Pattern to be searched, found inside <loc>(.*)</loc>\npattern = r\"loc>(.*)</loc\" urlPatterns = re.findall(pattern, page) #finding pattern on page\n\nfor url in urlPatterns: #iterating individual url inside urlPatterns\n```", "```py\nif re.match(r'.*blog', url): #Blog related\n    dataSetBlogURL.append(url)\n if re.match(r'[\\w\\-]', url):\n        blogTitle = re.findall(r'blog/([A-Za-z0-9\\-]+)', url)\n\n        if len(blogTitle) > 0 and not re.match('(category)', blogTitle[0]):\n            #blogTitle is a List, so index is applied.\n            dataSetBlog.append(blogTitle[0]) \n```", "```py\nprint(\"Blogs URL: \", len(dataSetBlogURL))\nprint(dataSetBlogURL)\n\nBlogs URL: 80\n['https://webscraping.com/blog', 'https://webscraping.com/blog/10/', \n'https://webscraping.com/blog/11/', .......,\n'https://webscraping.com/blog/category/screenshot', 'https://webscraping.com/blog/category/sitescraper', 'https://webscraping.com/blog/category/sqlite', 'https://webscraping.com/blog/category/user-agent', 'https://webscraping.com/blog/category/web2py', 'https://webscraping.com/blog/category/webkit', 'https://webscraping.com/blog/category/website/', 'https://webscraping.com/blog/category/xpath']\n```", "```py\n**print**(\"Blogs Title: \", len(dataSetBlog))\nprint(\"Unique Blog Count: \", len(set(dataSetBlog)))\nprint(dataSetBlog)\n#print(set(dataSetBlog)) #returns unique element from List similar to dataSetBlog.\n\nBlogs Title: 24\nUnique Blog Count: 24\n ['Android-Apps-Update', 'Apple-Apps-Update', 'Automating-CAPTCHAs', 'Automating-webkit', 'Bitcoin', 'Client-Feedback', 'Fixed-fee-or-hourly', 'Google-Storage', 'Google-interview', 'How-to-use-proxies', 'I-love-AJAX', 'Image-efficiencies', 'Luminati', 'Reverse-Geocode', 'Services', 'Solving-CAPTCHA', 'Startup', 'UPC-Database-Update', 'User-agents', 'Web-Scrapping', 'What-is-CSV', 'What-is-web-scraping', 'Why-Python', 'Why-web']\n```", "```py\nif re.match(r'.*category', url): #Category Related\n    dataSetCategoryURL.append(url)\n    categoryTitle = re.findall(r'category/([\\w\\s\\-]+)', url)\n    dataSetCategory.append(categoryTitle[0])\n\nprint(\"Category URL Count: \", len(dataSetCategoryURL))\nprint(dataSetCategoryURL)\n```", "```py\nCategory URL Count: 43\n['https://webscraping.com/blog/category/ajax', 'https://webscraping.com/blog/category/android/', 'https://webscraping.com/blog/category/big picture', 'https://webscraping.com/blog/category/business/', 'https://webscraping.com/blog/category/cache', 'https://webscraping.com/blog/category/captcha', ..................................., 'https://webscraping.com/blog/category/sitescraper', 'https://webscraping.com/blog/category/sqlite', 'https://webscraping.com/blog/category/user-agent', 'https://webscraping.com/blog/category/web2py', 'https://webscraping.com/blog/category/webkit', 'https://webscraping.com/blog/category/website/', 'https://webscraping.com/blog/category/xpath']\n```", "```py\nprint(\"Category Title Count: \", len(dataSetCategory))\nprint(\"Unique Category Count: \", len(set(dataSetCategory)))\nprint(dataSetCategory)\n#returns unique element from List similar to dataSetCategory.\n#print(set(dataSetCategory)) \n\nCategory Title Count: 43\nUnique Category Count: 43 \n['ajax', 'android', 'big picture', 'business', 'cache', 'captcha', 'chickenfoot', 'concurrent', 'cookies', 'crawling', 'database', 'efficiency', 'elance', 'example', 'flash', 'freelancing', 'gae', 'google', 'html', 'image', 'ip', 'ir', 'javascript', 'learn', 'linux', 'lxml', 'mobile', 'mobile apps', 'ocr', 'opensource', 'proxies', 'python', 'qt', 'regex', 'scrapy', 'screenshot', 'sitescraper', 'sqlite', 'user-agent', 'web2py', 'webkit', 'website', 'xpath']\n```"]