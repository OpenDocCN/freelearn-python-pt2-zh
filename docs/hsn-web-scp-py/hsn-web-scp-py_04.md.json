["```py\nC:\\> python \u2013version\nPython 3.7.0\n```", "```py\nC:\\> pip --version\n\npip 18.1 from c:\\python37\\lib\\site-packages\\pip (python 3.7)\n```", "```py\nC:\\> python -m pip install --upgrade pip\n```", "```py\nC:\\> pip install requests\n\nRequirement already satisfied: requests in c:\\python37\\lib\\site-packages (2.19.1)\n```", "```py\n>>> import urllib \n>>> help(urllib) #display documentation available for urllib\n\n```", "```py\nHelp on package urllib:\nNAME\n urllib\nPACKAGE CONTENTS\n error\n parse\n request\n response\n robotparser\nFILE\n c:\\python37\\lib\\urllib\\__init__.py\n```", "```py\n>>> import requests \n>>> requests.__version__ #display requests version \n\n'2.21.0'\n\n>>> help(requests)   #display documentation available for requests\n\nHelp on package requests:\nNAME\n requests\nDESCRIPTION\n Requests HTTP Library\n ~~~~~~~~~~~~~~~~~~\n Requests is an HTTP library, written in Python, for human beings.\n```", "```py\nModuleNotFoundError: No module named 'requests'\n```", "```py\nC:\\> pip install requests\n```", "```py\nC:\\> pip install requests -\u2013upgrade\n```", "```py\n>>> import urllib.request as req #import module request from urllib\n>>> link = \"https://en.wikipedia.org/wiki/List_of_most_popular_websites\"\n>>> response = req.urlopen(link)  #load the link using method urlopen()\n\n>>> print(type(response))   #print type of response object\n <class 'http.client.HTTPResponse'>\n\n>>> print(response.read()) #read response content\nb'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>List of most popular websites - Wikipedia</title>\\n<script>\u2026..,\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"List_of_most_popular_websites\",\"wgTitle\":\"List of most popular websites\",\u2026\u2026\n```", "```py\n>>> import requests\n>>> link = \"https://en.wikipedia.org/wiki/List_of_most_popular_websites\"\n>>> response = requests.get(link)\n\n>>> print(type(response))\n <class 'requests.models.Response'>\n\n>>> content = response.content #response content received\n>>> print(content[0:150])  #print(content) printing first 150 character from content\n\nb'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>List of most popular websites - Wikipedia</title>'\n```", "```py\n>>> import urllib.request \n\n>>> urllib.request.urlretrieve('https://www.samsclub.com/robots.txt')\n('C:\\\\Users\\\\*****\\AppData\\\\Local\\\\Temp\\\\tmpjs_cktnc', <http.client.HTTPMessage object at 0x04029110>)\n\n>>> urllib.request.urlretrieve(link,\"testrobots.txt\") #urlretrieve(url, filename=None)\n('testrobots.txt', <http.client.HTTPMessage object at 0x04322DF0>)\n```", "```py\n>>> import urllib.request\n>>> import os\n>>> content = urllib.request.urlopen('https://www.samsclub.com/robots.txt').read() #reads robots.txt content from provided URL\n\n>>> file = open(os.getcwd()+os.sep+\"contents\"+os.sep+\"robots.txt\",\"wb\") #Creating a file robots.txt inside directory 'contents' that exist under current working directory (os.getcwd()) \n\n>>> file.write(content) #writing content to file robots.txt opened in line above. If the file doesn't exist inside directory 'contents', Python will throw exception \"File not Found\"\n\n>>> file.close() #closes the file handle\n```", "```py\n>>> link=\"https://www.samsclub.com/sitemap.xml\"\n>>> import requests\n>>> content = requests.get(link).content\n>>> content \n\nb'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<sitemapindex >\\n<sitemap><loc>https://www.samsclub.com/sitemap_categories.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_products_1.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_products_2.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_locators.xml</loc></sitemap>\\n</sitemapindex>'\n\n>>> file = open(os.getcwd()+os.sep+\"contents\"+os.sep+\"sitemap.xml\",\"wb\") #Creating a file robots.txt inside directory 'contents' that exist under current working directory (os.getcwd()) \n\n>>> file.write(content) #writing content to file robots.txt opened in line above. If the file doesn't exist inside directory 'contents', Python will throw exception \"File not Found\"\n\n>>> file.close() #closes the file handle\n```", "```py\n>>> link=\"https://www.samsclub.com/sitemap.xml\"\n>>> import requests\n>>> content = requests.get(link).text  #using 'text'\n>>> content\n\n'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<sitemapindex >\\n<sitemap><loc>https://www.samsclub.com/sitemap_categories.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_products_1.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_products_2.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_locators.xml</loc></sitemap>\\n</sitemapindex>' >>> content = requests.get(link).content \n>>> content.decode() # decoding 'content' , decode('utf-8')\n\n'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<sitemapindex >\\n<sitemap><loc>https://www.samsclub.com/sitemap_categories.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_products_1.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_products_2.xml</loc></sitemap>\\n<sitemap><loc>https://www.samsclub.com/sitemap_locators.xml</loc></sitemap>\\n</sitemapindex>'\n```", "```py\n>>> import urllib.request\n>>> someRequest = urllib.request.urlopen(URL) #load/Open the URL\n>>> urllib.request.getheaders() #Lists all HTTP headers. \n\n>>> urllib.request.getheader(\"Content-Type\") #return value of header 'Content-Type'\n\n'text/html; charset=ISO-8859-1' or 'utf-8'\n```", "```py\n>>> import urllib.request\n>>> dir(urllib.request) #list features available from urllib.request\n\n['AbstractBasicAuthHandler', 'AbstractDigestAuthHandler', 'AbstractHTTPHandler', 'BaseHandler', 'CacheFTPHandler', 'ContentTooShortError', 'DataHandler', 'FTPHandler', 'FancyURLopener', 'FileHandler', 'HTTPBasicAuthHandler', 'HTTPCookieProcessor',....'Request', 'URLError', 'URLopener',......'pathname2url', 'posixpath', 'proxy_bypass', 'proxy_bypass_environment', 'proxy_bypass_registry', 'quote', 're', 'request_host', 'socket', 'splitattr', 'splithost', 'splitpasswd', 'splitport', 'splitquery', 'splittag', 'splittype', 'splituser', 'splitvalue', 'ssl', 'string', 'sys', 'tempfile', 'thishost', 'time', 'to_bytes', 'unquote', 'unquote_to_bytes', 'unwrap', 'url2pathname', 'urlcleanup', 'urljoin', 'urlopen', 'urlparse', 'urlretrieve', 'urlsplit', 'urlunparse', 'warnings']\n```", "```py\n>>> import urllib.request\n>>> link='https://www.google.com' [](https://www.google.com) \n>>> linkRequest = urllib.request.urlopen(link) #open link\n>>> print(type(linkRequest)) #object type\n <class 'http.client.HTTPResponse'> [](https://www.google.com) \n>>> linkResponse = urllib.request.urlopen(link).read() #open link and read content\n>>> print(type(linkResponse))\n <class 'bytes'>\n [](https://www.google.com) >>> requestObj = urllib.request.Request('https:/www.samsclub.com/robots.txt')\n>>> print(type(requestObj)) #object type\n <class 'urllib.request.Request'>\n\n>>> requestObjResponse = urllib.request.urlopen(requestObj).read()\n>>> print(type(requestObjResponse))  #object type\n <class 'bytes'>\n```", "```py\n>>> linkRequest.getcode()  #can also be used as: linkRequest.code or linkRequest.status \n\n 200\n```", "```py\n>>> linkRequest.geturl()   # can also be used as: linkRequest.url\n\n 'https://www.google.com'\n```", "```py\n>>> linkRequest._method \n'GET'\n```", "```py\n>>> linkRequest.getheaders()\n\n[('Date','Sun, 30 Dec 2018 07:00:25 GMT'),('Expires', '-1'),('Cache-Control','private, max-age=0'),('Content-Type','text/html; charset=ISO-8859-1'),('P3P', 'CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"'),('Server', 'gws'),('X-XSS-Protection', '1; mode=block'),('X-Frame-Options','SAMEORIGIN'),('Set-Cookie', '1P_JAR=\u2026..; expires=Tue, 29-Jan-2019 07:00:25 GMT; path=/; domain=.google.com'),('Set-Cookie 'NID=152=DANr9NtDzU_glKFRgVsOm2eJQpyLijpRav7OAAd97QXGX6WwYMC59dDPe.; expires=Mon, 01-Jul-2019 07:00:25 GMT; path=/; domain=.google.com; HttpOnly'),('Alt-Svc', 'quic=\":443\"; ma=2592000; v=\"44,43,39,35\"'),('Accept-Ranges', 'none'),('Vary', 'Accept-Encoding'),('Connection', 'close')] \n```", "```py\n>>> linkRequest.getheader(\"Content-Type\") \n\n 'text/html; charset=ISO-8859-1'\n\n>>> linkRequest.info()[\"content-type\"]\n\n 'text/html; charset=ISO-8859-1'\n```", "```py\n>>> import urllib.request as request\n>>> import urllib.error as error\n\n>>> try:  #attempting an error case\n request.urlopen(\"https://www.python.ogr\") #wrong URL is passed to urlopen()\n except error.URLError as e:\n print(\"Error Occurred: \",e.reason)\n\nError Occurred: [Errno 11001] getaddrinfo failed #output\n```", "```py\n>>> import urllib.parse as urlparse\n>>> print(dir(urlparse)) #listing features from urlparse\n```", "```py\n\n['DefragResult', 'DefragResultBytes', 'MAX_CACHE_SIZE', 'ParseResult', 'ParseResultBytes', 'Quoter', 'ResultBase', 'SplitResult', 'SplitResultBytes', .........'clear_cache', 'collections', 'namedtuple', 'non_hierarchical', 'parse_qs', 'parse_qsl', 'quote', 'quote_from_bytes', 'quote_plus', 're', 'scheme_chars', 'splitattr', 'splithost', 'splitnport', 'splitpasswd', 'splitport', 'splitquery', 'splittag', 'splittype', 'splituser', 'splitvalue', 'sys', 'to_bytes', 'unquote', 'unquote_plus', 'unquote_to_bytes', 'unwrap', 'urldefrag', 'urlencode', 'urljoin', 'urlparse', 'urlsplit', 'urlunparse', 'urlunsplit', 'uses_fragment', 'uses_netloc', 'uses_params', 'uses_query', 'uses_relative']\n```", "```py\n>>> amazonUrl ='https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dstripbooks-intl-ship&field-keywords=Packt+Books'\n\n>>> print(urlparse.urlsplit(amazonUrl)) #split amazonURL\nSplitResult(scheme='https', netloc='www.amazon.com', path='/s/ref=nb_sb_noss', query='url=search-alias%3Dstripbooks-intl-ship&field-keywords=Packt+Books', fragment='')\n\n>>> print(urlparse.urlsplit(amazonUrl).query) #query-string from amazonURL\n'url=search-alias%3Dstripbooks-intl-ship&field-keywords=Packt+Books'\n\n>>> print(urlparse.urlsplit(amazonUrl).scheme) #return URL scheme\n'https'\n```", "```py\n>>> print(urlparse.urlparse(amazonUrl)) #parsing components of amazonUrl\n\n ParseResult(scheme='https', netloc='www.amazon.com', path='/s/ref=nb_sb_noss', params='', query='url=search-alias%3Dstripbooks-intl-ship&field-keywords=Packt+Books', fragment='')\n```", "```py\nimport urllib.parse as urlparse\n>>> localUrl= 'http://localhost/programming/books;2018?browse=yes&sort=ASC#footer'\n\n>>> print(urlparse.urlsplit(localUrl))\nSplitResult(scheme='http', netloc='localhost', path='/programming/books;2018', query='browse=yes&sort=ASC', fragment='footer')\n\n>>> parseLink = urlparse.urlparse(localUrl)\nParseResult(scheme='http', netloc='localhost', path='/programming/books', params='2018', query='browse=yes&sort=ASC', fragment='footer')\n\n>>> print(parseLink.path) #path without domain information\n '/programming/books'\n\n>>> print(parseLink.params) #parameters \n '2018'\n\n>>> print(parseLink.fragment) #fragment information from URL\n 'footer'\n```", "```py\n>>> import urllib.parse as urlparse\n>>> data = {'param1': 'value1', 'param2': 'value2'}\n\n>>> urlparse.urlencode(data)\n 'param1=value1&param2=value2'\n\n>>> urlparse.parse_qs(urlparse.urlencode(data))\n {'param1': ['value1'], 'param2': ['value2']}\n\n>>> urlparse.urlencode(data).encode('utf-8')\n b'param1=value1&param2=value2'\n```", "```py\n>>> import urllib.parse as urlparse\n>>> url=\"http://localhost:8080/~cache/data file?id=1345322&display=yes&expiry=false\"\n\n>>> urlparse.quote(url) \n 'http%3A//localhost%3A8080/~cache/data%20file%3Fid%3D1345322%26display%3Dyes%26expiry%3Dfalse'\n\n>>> urlparse.unquote(url)\n 'http://localhost:8080/~cache/data file?id=1345322&display=yes&expiry=false'\n\n>>> urlparse.quote_plus(url) 'http%3A%2F%2Flocalhost%3A8080%2F~cache%2Fdata+file%3Fid%3D1345322%26display%3Dyes%26expiry%3Dfalse' \n\n>>> urlparse.unquote_plus(url)\n 'http://localhost:8080/~cache/data file?id=1345322&display=yes&expiry=false'\n```", "```py\n>>> import urllib.parse as urlparse\n\n>>> urlparse.urljoin('http://localhost:8080/~cache/','data file') #creating URL\n 'http://localhost:8080/~cache/data file'\n\n>>> urlparse.urljoin('http://localhost:8080/~cache/data file/','id=1345322&display=yes')\n 'http://localhost:8080/~cache/data file/id=1345322&display=yes'\n```", "```py\n>>> import urllib.robotparser as robot\n>>> par = robot.RobotFileParser()\n>>> par.set_url('https://www.samsclub.com/robots.txt') #setting robots URL\n>>> par.read()  #reading URL content\n\n>>> print(par)\nUser-agent: *\nAllow: /sams/account/signin/createSession.jsp\nDisallow: /cgi-bin/\nDisallow: /sams/checkout/\nDisallow: /sams/account/\nDisallow: /sams/cart/\nDisallow: /sams/eValues/clubInsiderOffers.jsp\nDisallow: /friend\nAllow: /sams/account/referal/\n\n>>> par.can_fetch('*','https://www.samsclub.com/category') #verify if URL is 'Allow' to Crawlers \nTrue\n\n>>> par.can_fetch('*','https://www.samsclub.com/friend')\nFalse\n```", "```py\n>>> import requests\n>>> link=\"http://www.python-requests.org\"\n>>> r = requests.get(link)\n\n>>> dir(r)\n['__attrs__', '__bool__', '__class__'......'_content', '_content_consumed', '_next', 'apparent_encoding', 'close', 'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history', 'is_permanent_redirect', 'is_redirect', 'iter_content', 'iter_lines', 'json', 'links', 'next', 'ok', 'raise_for_status', 'raw', 'reason', 'request', 'status_code', 'text', 'url']\n\n>>> print(type(r)) \n<class 'requests.models.Response'>\n```", "```py\n>>> r.url #URL of response object`\n 'http://www.python-requests.org/en/master/'\n\n>>> r.status_code #status code\n 200\n\n>>> r.history #status code of history event\n [<Response [302]>]\n```", "```py\n>>> r.headers #response headers with information about server, date.. \n{'Transfer-Encoding': 'chunked', 'Content-Type': 'text/html', 'Content-Encoding': 'gzip', 'Last-Modified': '....'Vary': 'Accept-Encoding', 'Server': 'nginx/1.14.0 (Ubuntu)', 'X-Cname-TryFiles': 'True', 'X-Served': 'Nginx', 'X-Deity': 'web02', 'Date': 'Tue, 01 Jan 2019 12:07:28 GMT'}\n\n>>> r.headers['Content-Type'] #specific header Content-Type\n 'text/html'\n\n>>> r.request.headers  #Request headers \n{'User-Agent': 'python-requests/2.21.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n\n>>> r.encoding  #response encoding\n 'ISO-8859-1'\n```", "```py\n>>> r.content[0:400]  #400 bytes characters\n\nb'\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\\n ....... <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\\n <title>Requests: HTTP for Humans\\xe2\\x84\\xa2 \u2014 Requests 2.21.0 documentation'\n\n>>> r.text[0:400]  #sub string that is 400 string character from response\n\n'\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\\n......\\n <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\\n <title>Requests: HTTP for Humans\u00e2\\x84\u00a2 \u2014 Requests 2.21.0 documentation'\n```", "```py\n>>> r = requests.get(link,stream=True) #raw response\n\n>>> print(type(r.raw))   #type of raw response obtained\n <class 'urllib3.response.HTTPResponse'>\n\n>>> r.raw.read(100)  #read first 100 character from raw response\n b\"\\x1f\\x8b\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\xed}[o\\xdcH\\x96\\xe6{\\xfe\\x8a\\xa8\\xd4\\xb4%O\\x8bL2/JI\\x96\\xb2Z\\x96e[U\\xbe\\xa8-\\xb9\\xaa\\x1b\\x85^!\\x92\\x8c\\xcc\\xa4\\xc5$Y\\xbc(\\x95\\xae)\\xa0\\x1e\\x06\\x18\\xcc\\xf3\\xce\\xcb\\x00\\xbbX`\\x16\\xd8\\xc7\\xc5>\\xed\\xeb\\x02\\xfb3f_\\x16\\xf5\\x0b\\xf6'\\xec9'\\x82\\x97\\xbc\\xc9\\xb2+#g\"\n```", "```py\n>>> import requests\n>>> link = \"https://feeds.citibikenyc.com/stations/stations.json\"\n>>> response = requests.get(link).json()\n\n>>> for i in range(10): #read 10 stationName from JSON response.\n print('Station ',response['stationBeanList'][i]['stationName'])\n\nStation W 52 St & 11 Ave\nStation Franklin St & W Broadway\nStation St James Pl & Pearl St\n........\nStation Clinton St & Joralemon St\nStation Nassau St & Navy St\nStation Hudson St & Reade St\n```", "```py\nimport requests\nlink=\"http://localhost:8080/~cache\"\n\nqueries= {'id':'123456','display':'yes'}\n\naddedheaders={'user-agent':''}\n\n#request made with parameters and headers\nr = requests.get(link, params=queries, headers=addedheaders) \nprint(r.url)\n```", "```py\nhttp://localhst:8080/~cache?id=123456+display=yes\n```", "```py\nimport requests pageUrl=\"http://httpbin.org/forms/post\"\npostUrl=\"http://httpbin.org/post\"\n\nparams = {'custname':'Mr. ABC','custtel':'','custemail':'abc@somedomain.com','size':'small', 'topping':['cheese','mushroom'],'delivery':'13:00','comments':'None'} headers={ 'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8','Content-Type':'application/x-www-form-urlencoded', 'Referer':pageUrl }\n\n#making POST request to postUrl with params and request headers, response will be read as JSON response = requests.post(postUrl,data=params,headers=headers).json()\nprint(response)\n```", "```py\n{\n'args': {}, \n'data': '', \n'files': {}, \n'form': {\n'comments': 'None', \n'custemail': 'abc@somedomain.com',\n'custname': 'Mr. ABC', \n'custtel': '',\n'delivery': '13:00', \n'size': 'small', \n'topping': ['cheese', 'mushroom']\n}, \n'headers': {    'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate', \n'Connection': 'close', \n'Content-Length': '130', \n'Content-Type': 'application/x-www-form-urlencoded', \n'Host': 'httpbin.org', \n'Referer': 'http://httpbin.org/forms/post', \n'User-Agent': 'python-requests/2.21.0'\n}, \n'json': None, 'origin': '202.51.76.90', \n'url': 'http://httpbin.org/post'\n}\n```"]